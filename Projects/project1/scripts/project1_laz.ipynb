{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** LS ****\n",
      "0.843705415289\n",
      "0.846162529975\n",
      "\t*** GS ****\n",
      "0.999983794006\n",
      "0.999983929033\n",
      "\t*** N ****\n",
      "0.999822772191\n",
      "0.999823159881\n",
      "\t*** CLS ****\n",
      "-mean weights:\n",
      "0.945358319202\n",
      "0.944921623947\n",
      "-best weights:\n",
      "0.945353938203\n",
      "0.944920581668\n",
      "\t**** Penalized *******\n",
      "0.999822772608\n",
      "0.999823160364\n"
     ]
    }
   ],
   "source": [
    "#Let's modify the data: replace -999 by -99\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -99)\n",
    "#PCA\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x_99.T, full_matrices=False)\n",
    "x_proj = np.dot(x_99, eigenvectors[:, 0:15])\n",
    "\n",
    "ratio = 0.1\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "\n",
    "tx_train_99, tr_mean_99, tr_std_99 = standardize(x_train)\n",
    "tx_test_99, te_mean_99, te_std_99 = standardize(x_test)\n",
    "\n",
    "#Least squares\n",
    "print(\"\\t*** LS ****\")\n",
    "w_LS, rmse_tr = least_squares(y_train, tx_train_99)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, w_LS)\n",
    "print(rmse_te)\n",
    "print(rmse_tr)\n",
    "\n",
    "#Gradient Descent\n",
    "print(\"\\t*** GS ****\")\n",
    "gamma = 0.00001\n",
    "max_iters = 5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_GD, rmse_tr_GD, = least_squares_GD(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_GD = compute_RMSE(y_test, tx_test_99, w_GD)\n",
    "print(rmse_te_GD)\n",
    "print(rmse_tr_GD)\n",
    "\n",
    "#Newton\n",
    "print(\"\\t*** N ****\")\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "lambd = 0.5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_N, rmse_tr_N = learning_by_newton_method(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_N = compute_RMSE(y_test, tx_test_99, w_N)\n",
    "print(rmse_te_N)\n",
    "print(rmse_tr_N)\n",
    "\n",
    "#Mean of 10 folds least squares\n",
    "print(\"\\t*** CLS ****\")\n",
    "k_folds = 10\n",
    "seed = 1\n",
    "w_LS_folds, rmse_tr_CLS, rmse_te_CLS = cross_validation_LS(y, x_proj, k_folds, seed)\n",
    "\n",
    "#Mean of weights along folds\n",
    "w_CLS = w_LS_folds.mean(axis=0)\n",
    "w_CLS = np.reshape(w_CLS, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_mean = compute_loss(y_test, tx_test_99, w_CLS)\n",
    "train_mse_CLS_mean = compute_loss(y_train, tx_train_99, w_CLS)\n",
    "rmse_te_CLS_mean = np.sqrt(2*test_mse_CLS_mean)\n",
    "rmse_tr_CLS_mean = np.sqrt(2*train_mse_CLS_mean)\n",
    "print(\"-mean weights:\")\n",
    "print(rmse_te_CLS_mean)\n",
    "print(rmse_tr_CLS_mean)\n",
    "\n",
    "#Best weights in test results\n",
    "w_CLS_best = w_LS_folds[np.argmin(rmse_te_CLS)]\n",
    "w_CLS_best = np.reshape(w_CLS_best, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_best = compute_loss(y_test, tx_test_99, w_CLS_best)\n",
    "train_mse_CLS_best = compute_loss(y_train, tx_train_99, w_CLS_best)\n",
    "rmse_te_CLS_best = np.sqrt(2*test_mse_CLS_best)\n",
    "rmse_tr_CLS_best = np.sqrt(2*train_mse_CLS_best)\n",
    "print(\"-best weights:\")\n",
    "print(rmse_te_CLS_best)\n",
    "print(rmse_tr_CLS_best)\n",
    "\n",
    "#Penalized Regression\n",
    "lambd = 0.05\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "print(\"\\t**** Penalized *******\")\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_reg, rmse_tr_reg = reg_logistic_regression(y_train, tx_train_99, lambd, initial_w, max_iters, gamma)\n",
    "rmse_te_reg = compute_RMSE(y_test, tx_test_99, w_reg)\n",
    "print(rmse_te_reg)\n",
    "print(rmse_tr_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10c95c940>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c95c9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,k_folds), rmse_tr_CLS)\n",
    "plt.plot(range(0,k_folds), rmse_te_CLS, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** IRLS *****\n",
      "48.0943754322\n",
      "47.6025668283\n"
     ]
    }
   ],
   "source": [
    "#IRLS\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -99)\n",
    "#PCA to avoid correlation and singular matrix\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x_99.T, full_matrices=False)\n",
    "x_proj = np.dot(x_99, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "ratio = 0.05\n",
    "x_train_PCA, x_test_PCA, y_train_PCA, y_test_PCA = split_data(x_proj, y, ratio)\n",
    "tx_train_PCA = np.c_[np.ones((y_train_PCA.shape[0], 1)), x_train_PCA]\n",
    "tx_test_PCA = np.c_[np.ones((y_test_PCA.shape[0], 1)), x_test_PCA]\n",
    "\n",
    "max_iters = 2\n",
    "print(\"\\t*** IRLS *****\")\n",
    "initial_w  = np.zeros((tx_train_PCA.shape[1],1))\n",
    "w_IRLS, rmse_tr_IRLS = learning_by_IRLS(y_train_PCA, tx_train_PCA, initial_w, max_iters)\n",
    "rmse_te_IRLS = compute_RMSE(y_test_PCA, tx_test_PCA, w_IRLS)\n",
    "print(rmse_te_IRLS)\n",
    "print(rmse_tr_IRLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (25000,16) and (1,) not aligned: 16 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bcbb44fcff5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_LS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstd_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_LS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrmse_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_RMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train_99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrmse_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_RMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test_99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mateusz/Dropbox/EPFL/Semester I/Machnie Learning CS-433/My Answers/MachineLearning/Projects/project1/scripts/costs.py\u001b[0m in \u001b[0;36mcompute_RMSE\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_RMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (25000,16) and (1,) not aligned: 16 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Not Sure what you wanted to achieve here\n",
    "mean_w = w_LS.mean(axis=0)\n",
    "std_w = w_LS.std(axis=0)\n",
    "rmse_tr = compute_RMSE(y_train, tx_train_99, mean_w)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, mean_w)\n",
    "print(rmse_tr)\n",
    "print(rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30)\n",
      "(30,)\n",
      "(30, 250000)\n",
      "(250000, 12)\n",
      "87.1532330429\n"
     ]
    }
   ],
   "source": [
    "x_s = standardize(x)\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:12])\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)\n",
    "print(V.shape)\n",
    "print(x_proj.shape)\n",
    "sigma = x_proj.std(axis=0).mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1d34dfb15a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_vec' is not defined"
     ]
    }
   ],
   "source": [
    "#Something is missing\n",
    "\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 30)\n",
      "(25000, 1)\n",
      "(225000, 30)\n",
      "(225000, 1)\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.1\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873645965557\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.001\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "initial_w  = np.zeros((tx_train.shape[1],1))\n",
    "\n",
    "w, rmse = learning_by_newton_method(y_train, tx_train, initial_w, 100, gamma)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateusz/Dropbox/EPFL/Semester I/Machnie Learning CS-433/My Answers/MachineLearning/Projects/project1/scripts/implementations.py:191: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "#alpha = 0.001\n",
    "lambd = 0.5\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-5, -1, 10)\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "for gamma in gammas:\n",
    "    initial_w  = np.zeros((tx_train.shape[1],1))\n",
    "    w, rmse = learning_by_newton_method(y_train, tx_train, initial_w, max_iter, gamma)\n",
    "#     lossREG = compute_loss(y_test, tx_test, w)\n",
    "#     rmse = np.sqrt(2*lossREG)\n",
    "    losses.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAETCAYAAAD6R0vDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoNLxt6QVeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d57beb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(gammas[0:5], losses[0:5], marker=\".\", color='b')\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_gradient_descent you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be deleted?\n",
    "best_weights = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be deleted?\n",
    "best_weights_std = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "try1, x_mean, std_x = standardize(x)\n",
    "#try1 = np.reshape(try1, (x.shape[0], x.shape[1]))\n",
    "print(try1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1608833.9247224757\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7100043066f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlambd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mw_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossTR_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossTE_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_laz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_vec' is not defined"
     ]
    }
   ],
   "source": [
    "weights = w\n",
    "print(\"RMSE: {l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "k_fold = 10\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation_laz(y_vec, x, k_fold, seed, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_newton_method you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "learning_by_newton_method() missing 1 required positional argument: 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cd66af3f344d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current iteration={i}, the loss={l}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: learning_by_newton_method() missing 1 required positional argument: 'gamma'"
     ]
    }
   ],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda done\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-493c9dd0557c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mw_RegPen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_laz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_LS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mateusz/Dropbox/EPFL/Semester I/Machnie Learning CS-433/My Answers/MachineLearning/Projects/project1/scripts/implementations.py\u001b[0m in \u001b[0;36mcross_validation_laz\u001b[0;34m(y, x, k_fold, seed, lambd, max_iter)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mweight_folds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "max_iter = 20\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "#w = np.ones((x_train.shape[1]+1, 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_RegPen, tr_rmse, te_rmse = cross_validation_laz(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenvectors, eigenvalues, V = np.linalg.svd(X_test.T, full_matrices=False)\n",
    "x_proj_test = np.dot(X_test, eigenvectors[:, 0:15])\n",
    "\n",
    "tX_test_proj = np.c_[np.ones((x_proj_test.shape[0], 1)), x_proj_test]\n",
    "\n",
    "weights = w_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test_proj)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
