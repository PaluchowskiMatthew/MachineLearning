{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def combinations(array2d, indeces_list_a, indeces_list_b):\n",
    "    combinations = list(itertools.product(indeces_list_a, indeces_list_b))\n",
    "    for comb in combinations:\n",
    "        new_feature = np.array([array2d[:,comb[0]] * array2d[:,comb[1]]]).T\n",
    "        array2d = np.hstack((array2d, new_feature))\n",
    "    return array2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's modify the data: replace -999 by -9\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -9)\n",
    "\n",
    "ratio = 0.1\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_99, y, ratio)\n",
    "\n",
    "#tx_train_99, tr_mean_99, tr_std_99 = standardize(x_train)\n",
    "#tx_test_99, te_mean_99, te_std_99 = standardize(x_test)\n",
    "\n",
    "tx_train_99 = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test_99 = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "#Least squares\n",
    "print(\"\\t*** LS ****\")\n",
    "w_LS, rmse_tr = least_squares(y_train, tx_train_99)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, w_LS)\n",
    "print(rmse_te)\n",
    "print(rmse_tr)\n",
    "\n",
    "#Gradient Descent\n",
    "print(\"\\t*** GS ****\")\n",
    "gamma = 0.00001\n",
    "max_iters = 5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_GD, rmse_tr_GD, = least_squares_GD(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_GD = compute_RMSE(y_test, tx_test_99, w_GD)\n",
    "print(rmse_te_GD)\n",
    "print(rmse_tr_GD)\n",
    "\n",
    "#Newton\n",
    "print(\"\\t*** N ****\")\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "lambd = 0.5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_N, rmse_tr_N = learning_by_newton_method(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_N = compute_RMSE(y_test, tx_test_99, w_N)\n",
    "print(rmse_te_N)\n",
    "print(rmse_tr_N)\n",
    "\n",
    "#Mean of 10 folds least squares\n",
    "print(\"\\t*** CLS ****\")\n",
    "k_folds = 10\n",
    "seed = 2\n",
    "w_LS_folds, rmse_tr_CLS, rmse_te_CLS = cross_validation_LS(y, x_99, k_folds, seed)\n",
    "\n",
    "#Mean of weights along folds\n",
    "w_CLS = w_LS_folds.mean(axis=0)\n",
    "w_CLS = np.reshape(w_CLS, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_mean = compute_loss(y_test, tx_test_99, w_CLS)\n",
    "train_mse_CLS_mean = compute_loss(y_train, tx_train_99, w_CLS)\n",
    "rmse_te_CLS_mean = np.sqrt(2*test_mse_CLS_mean)\n",
    "rmse_tr_CLS_mean = np.sqrt(2*train_mse_CLS_mean)\n",
    "print(\"-mean weights:\")\n",
    "print(rmse_te_CLS_mean)\n",
    "print(rmse_tr_CLS_mean)\n",
    "\n",
    "#Best weights in test results\n",
    "w_CLS_best = w_LS_folds[np.argmin(rmse_te_CLS)]\n",
    "w_CLS_best = np.reshape(w_CLS_best, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_best = compute_loss(y_test, tx_test_99, w_CLS_best)\n",
    "train_mse_CLS_best = compute_loss(y_train, tx_train_99, w_CLS_best)\n",
    "rmse_te_CLS_best = np.sqrt(2*test_mse_CLS_best)\n",
    "rmse_tr_CLS_best = np.sqrt(2*train_mse_CLS_best)\n",
    "print(\"-best weights:\")\n",
    "print(rmse_te_CLS_best)\n",
    "print(rmse_tr_CLS_best)\n",
    "\n",
    "#Penalized Regression\n",
    "lambd = 0.5\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "print(\"\\t**** Penalized *******\")\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_reg, rmse_tr_reg = reg_logistic_regression(y_train, tx_train_99, lambd, initial_w, max_iters, gamma)\n",
    "rmse_te_reg = compute_RMSE(y_test, tx_test_99, w_reg)\n",
    "print(rmse_te_reg)\n",
    "print(rmse_tr_reg)\n",
    "\n",
    "#kind of F1-measure\n",
    "print(\"ratio of misclassified over total predictions: \")\n",
    "\n",
    "y_pred_LS = predict_labels(w_CLS, tx_test_99)\n",
    "f1_CLS = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "print(\"F1 CLS: {l}\".format(l=1-f1_CLS))\n",
    "\n",
    "y_pred_GS = predict_labels(w_GD, tx_test_99)\n",
    "f1_GS = sum(abs(y_test-y_pred_GS))/(2*len(y_pred_GS))\n",
    "print(\"F1 GS: {l}\".format(l=1-f1_GS))\n",
    "\n",
    "y_pred_N = predict_labels(w_N, tx_test_99)\n",
    "f1_N = sum(abs(y_test-y_pred_N))/(2*len(y_pred_N))\n",
    "print(\"F1 N: {l}\".format(l=1-f1_N))\n",
    "\n",
    "y_pred_PR = predict_labels(w_reg, tx_test_99)\n",
    "f1_PR = sum(abs(y_test-y_pred_PR))/(2*len(y_pred_PR))\n",
    "print(\"F1 PR: {l}\".format(l=1-f1_PR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,k_folds), rmse_tr_CLS)\n",
    "plt.plot(range(0,k_folds), rmse_te_CLS, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot variance and mean of test/train error of Least squares\n",
    "plt.boxplot([rmse_te_CLS, rmse_tr_CLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#***** RIDGE AND LEAST SQUARE ON POLYNOMIAL BASIS FUNCTION DATASET\n",
    "\n",
    "#Let's replace the -999 by Nan to avoid computation while computing the power 3\n",
    "x_nan = x.copy()\n",
    "np.putmask(x_nan, x_nan==-999, np.nan)\n",
    "x_square = np.zeros((x_nan.shape[0], 3*x_nan.shape[1]))\n",
    "\n",
    "#Let's build a matrix using a polynomial basis function (power 3)\n",
    "for column in range(0,x_nan.shape[1]):\n",
    "    x_square[:, column] = x_nan[:, column]\n",
    "    x_square[:, column+x_nan.shape[1]] = np.multiply(x_nan[:,column], x_nan[:,column])\n",
    "    x_square[:, column+2*x_nan.shape[1]] = np.multiply(np.multiply(x_nan[:,column], x_nan[:,column]), x_nan[:,column])\n",
    "#put back -9 instead of Nan\n",
    "x_square[np.isnan(x_square)]=-9\n",
    "\n",
    "ratio = 0.3\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_square, y, ratio)\n",
    "\n",
    "#tx_train_square, m, s = standardize(x_train)\n",
    "#tx_test_square, m, s = standardize(x_test)\n",
    "\n",
    "tx_train_square = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test_square = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.ones((tx_train_square.shape[1], 1))\n",
    "\n",
    "#Calculate accuracy with a least square regression\n",
    "w_LS_pow3, rmse_LS_pow3 = least_squares(y_train, tx_train_square)\n",
    "y_pred_LS = predict_labels(w_LS_pow3, tx_test_square)\n",
    "f1_LS_pow3 = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "print(\"f1 score for LS: {f}\".format(f=1-f1_LS_pow3))\n",
    "\n",
    "#calculate accuracy for different lambdas (use cross_validation_ridge() for cross folds)\n",
    "lambds = np.logspace(-6, -5, 50)\n",
    "best_f1 = 1000\n",
    "best_lamb = 0\n",
    "k_folds = 5\n",
    "seed = 1\n",
    "f1_mean = []\n",
    "f1_std = []\n",
    "for lamb in lambds:\n",
    "    w_rid, r = ridge_regression(y_train, tx_train_square, lamb)\n",
    "    y_pred_rid = predict_labels(w_rid, tx_test_square)\n",
    "    f1_rid = sum(abs(y_test-y_pred_rid))/(2*len(y_pred_rid))\n",
    "    if f1_rid < best_f1:\n",
    "        best_lamb = lamb\n",
    "        best_f1 = f1_rid\n",
    "\n",
    "print(\"the best lmabda is {bl} with {f}\".format(bl=best_lamb, f=1-best_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx_total= np.c_[np.ones((y.shape[0], 1)), x_square]\n",
    "\n",
    "y_pred_mean = predict_labels(w_LS_pow3, tx_total)\n",
    "y_pred_mean = np.reshape(y_pred_mean, (len(y_pred_mean), 1))\n",
    "f1_rid_mean = sum(abs(y-y_pred_mean))/(2*len(y_pred_mean))\n",
    "print(1-f1_rid_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_rid_mean = weights_ridge.mean(axis=0)\n",
    "w_rid_best = weights_ridge[np.argmin(f1_ridge_folds)]\n",
    "\n",
    "tx_total, m, s = standardize(x_square)\n",
    "\n",
    "y_pred_mean = predict_labels(w_rid_mean, tx_total)\n",
    "y_pred_mean = np.reshape(y_pred_mean, (len(y_pred_mean), 1))\n",
    "f1_rid_mean = sum(abs(y-y_pred_mean))/(2*len(y_pred_mean))\n",
    "\n",
    "print(1-f1_rid_mean)\n",
    "\n",
    "y_pred_best = predict_labels(w_rid_best, tx_total)\n",
    "y_pred_best = np.reshape(y_pred_best, (len(y_pred_best), 1))\n",
    "f1_rid_best = sum(abs(y-y_pred_best))/(2*len(y_pred_best))\n",
    "\n",
    "print(1-f1_rid_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#***** RIDGE AND LEAST SQUARE ON POLYNOMIAL BASIS FUNCTION DATASET\n",
    "#************ POWER 9 **********************\n",
    "\n",
    "#Let's replace the -999 by Nan to avoid computation while computing the power 3\n",
    "x_nan = x.copy()\n",
    "np.putmask(x_nan, x_nan==-999, np.nan)\n",
    "degree = 9\n",
    "x_fifth = np.zeros((x_nan.shape[0], degree*x_nan.shape[1]))\n",
    "\n",
    "#Let's build a matrix using a polynomial basis function (power 5)\n",
    "for column in range(0,x_nan.shape[1]):\n",
    "    x_fifth[:, column] = x_nan[:, column]\n",
    "    for deg in range(1,degree):\n",
    "        x_fifth[:, column + deg*x_nan.shape[1]] = np.multiply(x_nan[:, column], x_fifth[:, column + (deg-1)*x_nan.shape[1]])\n",
    "\n",
    "#put back -9 instead of Nan\n",
    "x_fifth[np.isnan(x_fifth)]=-9\n",
    "\n",
    "ratio = 0.2\n",
    "seed = 2\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_fifth, y, ratio, seed)\n",
    "\n",
    "#tx_train_fifth, m, s = standardize(x_train)\n",
    "#tx_test_fifth, m, s = standardize(x_test)\n",
    "\n",
    "tx_train_fifth = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test_fifth = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.ones((tx_train_fifth.shape[1], 1))\n",
    "\n",
    "w_LS_pow5, rmse_LS_pow5 = least_squares(y_train, tx_train_fifth)\n",
    "y_pred_LS = predict_labels(w_LS_pow5, tx_test_fifth)\n",
    "f1_LS_pow5 = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "print(\"f1 score for LS: {f}\".format(f=1-f1_LS_pow5))\n",
    "\n",
    "\n",
    "#calculate accuracy for different lambdas (use cross_validation_ridge() for cross folds)\n",
    "lambds = np.logspace(-7, -2, 100)\n",
    "best_f1 = 0\n",
    "best_lamb = 0\n",
    "k_folds = 10\n",
    "seed = 1\n",
    "f1_tr = []\n",
    "f1_te = []\n",
    "for lamb in lambds:\n",
    "    w_rid, r = ridge_regression(y_train, tx_train_fifth, lamb)\n",
    "    y_pred_tr = predict_labels(w_rid, tx_train_fifth)\n",
    "    y_pred_te = predict_labels(w_rid, tx_test_fifth)\n",
    "    f1_rid_tr = 1-sum(abs(y_train-y_pred_tr))/(2*len(y_pred_tr))\n",
    "    f1_rid_te = 1-sum(abs(y_test-y_pred_te))/(2*len(y_pred_te))\n",
    "    f1_tr.append(f1_rid_tr)\n",
    "    f1_te.append(f1_rid_te)\n",
    "    if f1_rid_te > best_f1:\n",
    "        best_lamb = lamb\n",
    "        best_f1 = f1_rid_te\n",
    "\n",
    "#Print and plot results\n",
    "print(\"the best lmabda is {bl} with {f}\".format(bl=best_lamb, f=best_f1))\n",
    "plt.figure()\n",
    "plt.semilogx(lambds, f1_tr)\n",
    "plt.semilogx(lambds, f1_te, color='r')\n",
    "plt.xlabel(\"Penalization lambda\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy of prediction\",  fontsize=20)\n",
    "plt.legend(('Train', 'Test'),  fontsize=20)\n",
    "plt.title(\"Influence of lambda\", fontsize=20)\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cross validation on Ridge and least squares\n",
    "k_folds = 10\n",
    "\n",
    "f1, w_CV_rid = cross_validation_ridge(y_train, x_train, best_lamb, k_folds, seed)\n",
    "\n",
    "w_CV_LS, re, rt = cross_validation_LS(y_train, x_train, k_folds, seed)\n",
    "\n",
    "f1_CV_rid = []\n",
    "f1_CV_LS = []\n",
    "print(\"cross folds: ridge\")\n",
    "for i in range(0, w_CV_rid.shape[0]):\n",
    "    y_pred_rid = predict_labels(w_CV_rid[i], tx_test_fifth)\n",
    "    y_pred_rid = np.reshape(y_pred_rid, (len(y_pred_rid), 1))\n",
    "    f1_rid = sum(abs(y_test-y_pred_rid))/(2*len(y_pred_rid))\n",
    "    print(1-f1_rid)\n",
    "    f1_CV_rid.append(1-f1_rid)\n",
    "    \n",
    "print(\"cross folds: LS\")\n",
    "for i in range(0, w_CV_LS.shape[0]):\n",
    "    y_pred_LS = predict_labels(w_CV_LS[i], tx_test_fifth)\n",
    "    y_pred_LS = np.reshape(y_pred_LS, (len(y_pred_LS), 1))\n",
    "    f1_LS = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "    print(1-f1_LS)\n",
    "    f1_CV_LS.append(1-f1_LS)\n",
    "    \n",
    "f1_CV_LS = np.reshape(f1_CV_LS, (len(f1_CV_LS), 1))\n",
    "f1_CV_rid = np.reshape(f1_CV_rid, (len(f1_CV_rid), 1))\n",
    "\n",
    "\n",
    "plt.boxplot([f1_CV_LS, f1_CV_rid])\n",
    "plt.title('Mean and Variance over 10 folds')\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Accuracy of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best lmabda is 2.2570197196339216e-07 with [ 0.81594]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAErCAYAAACYQVdCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nasK/CznIr48sMJEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6664e2bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#***** POLYNOMIAL AND FEATURE COMBINATION\n",
    "#************ POWER 9 **********************\n",
    "\n",
    "#Let's replace the -999 by Nan to avoid computation while computing the power 3\n",
    "x_nan = x.copy()\n",
    "np.putmask(x_nan, x_nan==-999, np.nan)\n",
    "degree = 9\n",
    "x_pow9 = np.zeros((x_nan.shape[0], degree*x_nan.shape[1]+53))\n",
    "\n",
    "#Let's build a matrix using a polynomial basis function (power 5)\n",
    "for column in range(0,x_nan.shape[1]):\n",
    "    x_pow9[:, column] = x_nan[:, column]\n",
    "    for deg in range(1,degree):\n",
    "        x_pow9[:, column + deg*x_nan.shape[1]] = np.multiply(x_nan[:, column], x_pow9[:, column + (deg-1)*x_nan.shape[1]])\n",
    "        \n",
    "x_pow9 = combinations(x_pow9, [0], [1,2,6,7,])\n",
    "x_pow9 = combinations(x_pow9, [2], [6,7])\n",
    "x_pow9 = combinations(x_pow9, [3], [6,9,19,21,23,29])\n",
    "x_pow9 = combinations(x_pow9, [4], [5,6])\n",
    "x_pow9 = combinations(x_pow9, [5], [6])\n",
    "x_pow9 = combinations(x_pow9, [6], [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29])\n",
    "x_pow9 = combinations(x_pow9, [9], [10,13,16,19,21,23,26,29])\n",
    "x_pow9 = combinations(x_pow9, [10], [13,16])\n",
    "x_pow9 = combinations(x_pow9, [21], [23,26,29])\n",
    "x_pow9 = combinations(x_pow9, [23], [26,29])\n",
    "x_pow9 = combinations(x_pow9, [26], [29])\n",
    "\n",
    "#put back -9 instead of Nan\n",
    "x_pow9[np.isnan(x_pow9)]=-9\n",
    "\n",
    "ratio = 0.2\n",
    "seed = 2\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_pow9, y, ratio, seed)\n",
    "\n",
    "#tx_train_fifth, m, s = standardize(x_train)\n",
    "#tx_test_fifth, m, s = standardize(x_test)\n",
    "\n",
    "tx_train_pow9 = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test_pow9 = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "w = np.ones((tx_train_pow9.shape[1], 1))\n",
    "\n",
    "#calculate accuracy for different lambdas (use cross_validation_ridge() for cross folds)\n",
    "lambds = np.logspace(-7, -2, 100)\n",
    "best_f1 = 0\n",
    "best_lamb = 0\n",
    "k_folds = 10\n",
    "seed = 1\n",
    "f1_tr = []\n",
    "f1_te = []\n",
    "for lamb in lambds:\n",
    "    w_rid, r = ridge_regression(y_train, tx_train_pow9, lamb)\n",
    "    y_pred_tr = predict_labels(w_rid, tx_train_pow9)\n",
    "    y_pred_te = predict_labels(w_rid, tx_test_pow9)\n",
    "    f1_rid_tr = 1-sum(abs(y_train-y_pred_tr))/(2*len(y_pred_tr))\n",
    "    f1_rid_te = 1-sum(abs(y_test-y_pred_te))/(2*len(y_pred_te))\n",
    "    f1_tr.append(f1_rid_tr)\n",
    "    f1_te.append(f1_rid_te)\n",
    "    if f1_rid_te > best_f1:\n",
    "        best_lamb = lamb\n",
    "        best_f1 = f1_rid_te\n",
    "\n",
    "#Print and plot results\n",
    "print(\"the best lmabda is {bl} with {f}\".format(bl=best_lamb, f=best_f1))\n",
    "plt.figure()\n",
    "plt.semilogx(lambds, f1_tr)\n",
    "plt.semilogx(lambds, f1_te, color='r')\n",
    "plt.xlabel(\"Penalization lambda\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy of prediction\",  fontsize=20)\n",
    "plt.legend(('Train', 'Test'),  fontsize=20)\n",
    "plt.title(\"Influence of lambda\", fontsize=20)\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross folds: ridge\n",
      "[ 0.815965]\n",
      "[ 0.815535]\n",
      "[ 0.81566]\n",
      "[ 0.81562]\n",
      "[ 0.81557]\n",
      "[ 0.81585]\n",
      "[ 0.815385]\n",
      "[ 0.815765]\n",
      "[ 0.815725]\n",
      "[ 0.81585]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f66439bee48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEeCAYAAADFHWEmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66439aee48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cross validation on Ridge and least squares\n",
    "k_folds = 10\n",
    "\n",
    "f1, w_CV_rid = cross_validation_ridge(y_train, x_train, best_lamb, k_folds, seed)\n",
    "\n",
    "f1_CV_rid = []\n",
    "f1_CV_LS = []\n",
    "print(\"cross folds: ridge\")\n",
    "for i in range(0, w_CV_rid.shape[0]):\n",
    "    y_pred_rid = predict_labels(w_CV_rid[i], tx_test_pow9)\n",
    "    y_pred_rid = np.reshape(y_pred_rid, (len(y_pred_rid), 1))\n",
    "    f1_rid = sum(abs(y_test-y_pred_rid))/(2*len(y_pred_rid))\n",
    "    print(1-f1_rid)\n",
    "    f1_CV_rid.append(1-f1_rid)\n",
    "\n",
    "f1_CV_rid = np.reshape(f1_CV_rid, (len(f1_CV_rid), 1))\n",
    "\n",
    "plt.boxplot([f1_CV_LS, f1_CV_rid])\n",
    "plt.title('Mean and Variance over 10 folds')\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Accuracy of prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Something is missing\n",
    "\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "initial_w  = np.zeros((tx_train.shape[1],1))\n",
    "\n",
    "w, rmse = learning_by_newton_method(y_train, tx_train, initial_w, 100, gamma)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "#alpha = 0.001\n",
    "lambd = 0.1\n",
    "ratio = 0.05\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-5, -1, 10)\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_99, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "for gamma in gammas:\n",
    "    initial_w  = np.ones((tx_train.shape[1],1))\n",
    "    w_NM, rmse_NM = learning_by_newton_method(y_train, tx_train, initial_w, max_iter, gamma)\n",
    "#     lossREG = compute_loss(y_test, tx_test, w)\n",
    "#     rmse = np.sqrt(2*lossREG)\n",
    "    losses.append(rmse_NM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(gammas[0:5], losses[0:5], marker=\".\", color='b')\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_gradient_descent you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 20\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "max_iters = 5\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation_laz(y, x_99, k_fold, seed, lambd, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_newton_method you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "#w = np.ones((x_train.shape[1]+1, 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_RegPen, tr_rmse, te_rmse = cross_validation_laz(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "#Let's replace the -999 by Nan to avoid computation while computing the power 3\n",
    "x_nan_test = X_test.copy()\n",
    "np.putmask(x_nan_test, x_nan_test==-999, np.nan)\n",
    "degree = 9\n",
    "x_TEST = np.zeros((x_nan_test.shape[0], degree*x_nan_test.shape[1]+53))\n",
    "\n",
    "#Let's build a matrix using a polynomial basis function (power 5)\n",
    "for column in range(0,x_nan_test.shape[1]):\n",
    "    x_TEST[:, column] = x_nan_test[:, column]\n",
    "    for deg in range(1,degree):\n",
    "        x_TEST[:, column + deg*x_nan_test.shape[1]] = np.multiply(x_nan_test[:, column], x_TEST[:, column + (deg-1)*x_nan_test.shape[1]])\n",
    "        \n",
    "x_TEST = combinations(x_TEST, [0], [1,2,6,7,])\n",
    "x_TEST = combinations(x_TEST, [2], [6,7])\n",
    "x_TEST = combinations(x_TEST, [3], [6,9,19,21,23,29])\n",
    "x_TEST = combinations(x_TEST, [4], [5,6])\n",
    "x_TEST = combinations(x_TEST, [5], [6])\n",
    "x_TEST = combinations(x_TEST, [6], [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29])\n",
    "x_TEST = combinations(x_TEST, [9], [10,13,16,19,21,23,26,29])\n",
    "x_TEST = combinations(x_TEST, [10], [13,16])\n",
    "x_TEST = combinations(x_TEST, [21], [23,26,29])\n",
    "x_TEST = combinations(x_TEST, [23], [26,29])\n",
    "x_TEST = combinations(x_TEST, [26], [29])\n",
    "\n",
    "#put back -9 instead of Nan\n",
    "x_TEST[np.isnan(x_TEST)]=-9\n",
    "\n",
    "tx_TEST = np.c_[np.ones((x_TEST.shape[0], 1)), x_TEST]\n",
    "print(tx_TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377,)\n"
     ]
    }
   ],
   "source": [
    "weights = w_CV_rid[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/ridgeA815.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tx_TEST)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
