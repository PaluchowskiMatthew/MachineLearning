{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** LS ****\n",
      "0.824319370683\n",
      "0.826956343995\n",
      "\t*** GS ****\n",
      "0.928404908633\n",
      "0.928677901986\n",
      "\t*** N ****\n",
      "0.999809002735\n",
      "0.999810306918\n",
      "\t*** CLS ****\n",
      "-mean weights:\n",
      "0.823861760302\n",
      "0.82732031017\n",
      "-best weights:\n",
      "0.823867919796\n",
      "0.827321865538\n",
      "\t**** Penalized *******\n",
      "0.999808995056\n",
      "0.999810331214\n",
      "ratio of misclassified over total predictions: \n",
      "F1 CLS: [ 0.25511556]\n",
      "F1 GS: [ 0.30983111]\n",
      "F1 N: [ 0.31756444]\n",
      "F1 PR: [ 0.31756]\n"
     ]
    }
   ],
   "source": [
    "#Let's modify the data: replace -999 by -9\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -9)\n",
    "\n",
    "ratio = 0.1\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_99, y, ratio)\n",
    "\n",
    "#tx_train_99, tr_mean_99, tr_std_99 = standardize(x_train)\n",
    "#tx_test_99, te_mean_99, te_std_99 = standardize(x_test)\n",
    "\n",
    "tx_train_99 = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test_99 = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "#Least squares\n",
    "print(\"\\t*** LS ****\")\n",
    "w_LS, rmse_tr = least_squares(y_train, tx_train_99)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, w_LS)\n",
    "print(rmse_te)\n",
    "print(rmse_tr)\n",
    "\n",
    "#Gradient Descent\n",
    "print(\"\\t*** GS ****\")\n",
    "gamma = 0.00001\n",
    "max_iters = 5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_GD, rmse_tr_GD, = least_squares_GD(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_GD = compute_RMSE(y_test, tx_test_99, w_GD)\n",
    "print(rmse_te_GD)\n",
    "print(rmse_tr_GD)\n",
    "\n",
    "#Newton\n",
    "print(\"\\t*** N ****\")\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "lambd = 0.5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_N, rmse_tr_N = learning_by_newton_method(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_N = compute_RMSE(y_test, tx_test_99, w_N)\n",
    "print(rmse_te_N)\n",
    "print(rmse_tr_N)\n",
    "\n",
    "#Mean of 10 folds least squares\n",
    "print(\"\\t*** CLS ****\")\n",
    "k_folds = 10\n",
    "seed = 2\n",
    "w_LS_folds, rmse_tr_CLS, rmse_te_CLS = cross_validation_LS(y, x_99, k_folds, seed)\n",
    "\n",
    "#Mean of weights along folds\n",
    "w_CLS = w_LS_folds.mean(axis=0)\n",
    "w_CLS = np.reshape(w_CLS, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_mean = compute_loss(y_test, tx_test_99, w_CLS)\n",
    "train_mse_CLS_mean = compute_loss(y_train, tx_train_99, w_CLS)\n",
    "rmse_te_CLS_mean = np.sqrt(2*test_mse_CLS_mean)\n",
    "rmse_tr_CLS_mean = np.sqrt(2*train_mse_CLS_mean)\n",
    "print(\"-mean weights:\")\n",
    "print(rmse_te_CLS_mean)\n",
    "print(rmse_tr_CLS_mean)\n",
    "\n",
    "#Best weights in test results\n",
    "w_CLS_best = w_LS_folds[np.argmin(rmse_te_CLS)]\n",
    "w_CLS_best = np.reshape(w_CLS_best, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_best = compute_loss(y_test, tx_test_99, w_CLS_best)\n",
    "train_mse_CLS_best = compute_loss(y_train, tx_train_99, w_CLS_best)\n",
    "rmse_te_CLS_best = np.sqrt(2*test_mse_CLS_best)\n",
    "rmse_tr_CLS_best = np.sqrt(2*train_mse_CLS_best)\n",
    "print(\"-best weights:\")\n",
    "print(rmse_te_CLS_best)\n",
    "print(rmse_tr_CLS_best)\n",
    "\n",
    "#Penalized Regression\n",
    "lambd = 0.5\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "print(\"\\t**** Penalized *******\")\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_reg, rmse_tr_reg = reg_logistic_regression(y_train, tx_train_99, lambd, initial_w, max_iters, gamma)\n",
    "rmse_te_reg = compute_RMSE(y_test, tx_test_99, w_reg)\n",
    "print(rmse_te_reg)\n",
    "print(rmse_tr_reg)\n",
    "\n",
    "#kind of F1-measure\n",
    "print(\"ratio of misclassified over total predictions: \")\n",
    "\n",
    "y_pred_LS = predict_labels(w_CLS, tx_test_99)\n",
    "f1_CLS = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "print(\"F1 CLS: {l}\".format(l=f1_CLS))\n",
    "\n",
    "y_pred_GS = predict_labels(w_GD, tx_test_99)\n",
    "f1_GS = sum(abs(y_test-y_pred_GS))/(2*len(y_pred_GS))\n",
    "print(\"F1 GS: {l}\".format(l=f1_GS))\n",
    "\n",
    "y_pred_N = predict_labels(w_N, tx_test_99)\n",
    "f1_N = sum(abs(y_test-y_pred_N))/(2*len(y_pred_N))\n",
    "print(\"F1 N: {l}\".format(l=f1_N))\n",
    "\n",
    "y_pred_PR = predict_labels(w_reg, tx_test_99)\n",
    "f1_PR = sum(abs(y_test-y_pred_PR))/(2*len(y_pred_PR))\n",
    "print(\"F1 PR: {l}\".format(l=f1_PR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,k_folds), rmse_tr_CLS)\n",
    "plt.plot(range(0,k_folds), rmse_te_CLS, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot variance and mean of test/train error of Least squares\n",
    "plt.boxplot([rmse_te_CLS, rmse_tr_CLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_nan = x.copy()\n",
    "np.putmask(x_nan, x_nan==-9, np.nan)\n",
    "x_square = np.zeros((x_nan.shape[0], 2*x_nan.shape[1]))\n",
    "\n",
    "for column in range(0,x_nan.shape[1]):\n",
    "    x_square[:, column] = x_nan[:, column]\n",
    "    x_square[:, column+x_nan.shape[1]] = np.multiply(x_nan[:,column], x_nan[:,column])\n",
    "\n",
    "np.putmask(x_square, x_square==np.nan, -9)\n",
    "\n",
    "ratio = 0.4\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_square, y, ratio)\n",
    "\n",
    "tx_train_square, m, s = standardize(x_train)\n",
    "tx_test_square, m, s = standardize(x_test)\n",
    "\n",
    "#tx_train_square = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test_square = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.ones((tx_train_square.shape[1], 1))\n",
    "\n",
    "lambds = np.logspace(-5, -2, 10)\n",
    "for lamb in lambds:\n",
    "    w_NM, rmse_NM = learning_by_newton_method(y_train, tx_train_square, w, 5, lamb)\n",
    "    rmse_NM_tr_s = compute_RMSE(y_train, tx_train_square, w_NM)\n",
    "    rmse_NM_te_s = compute_RMSE(y_test, tx_test_square, w_NM)\n",
    "    print(rmse_NM_tr_s)\n",
    "    print(rmse_NM_te_s)\n",
    "    print(\"************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_NM, rmse_NM = logistic_regression(y_train, tx_train_square, w, 10, 0.01)\n",
    "y_pred_test = predict_labels(w_NM, tx_test_square)\n",
    "sum(abs(y_test-y_pred_test))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IRLS\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -99)\n",
    "#PCA to avoid correlation and singular matrix\n",
    "#eigenvectors, eigenvalues, V = np.linalg.svd(x_99.T, full_matrices=False)\n",
    "#x_proj = np.dot(x_99, eigenvectors[:, 0:10])\n",
    "#y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "ratio = 0.05\n",
    "x_train_IRLS, x_test_IRLS, y_train_IRLS, y_test_IRLS = split_data(x_99, y, ratio)\n",
    "tx_train_IRLS = np.c_[np.ones((y_train_IRLS.shape[0], 1)), x_train_IRLS]\n",
    "tx_test_IRLS= np.c_[np.ones((y_test_IRLS.shape[0], 1)), x_test_IRLS]\n",
    "\n",
    "max_iters = 5\n",
    "print(\"\\t*** IRLS *****\")\n",
    "initial_w  = np.ones((tx_train_IRLS.shape[1],1))\n",
    "w_IRLS, rmse_tr_IRLS = learning_by_IRLS(y_train_IRLS, tx_train_IRLS, initial_w, max_iters)\n",
    "rmse_te_IRLS = compute_RMSE(y_test_IRLS, tx_test_IRLS, w_IRLS)\n",
    "print(rmse_te_IRLS)\n",
    "print(rmse_tr_IRLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Not Sure what you wanted to achieve here\n",
    "mean_w = w_LS.mean(axis=0)\n",
    "std_w = w_LS.std(axis=0)\n",
    "rmse_tr = compute_RMSE(y_train, tx_train_99, mean_w)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, mean_w)\n",
    "print(rmse_tr)\n",
    "print(rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_s = standardize(x)\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:12])\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)\n",
    "print(V.shape)\n",
    "print(x_proj.shape)\n",
    "sigma = x_proj.std(axis=0).mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Something is missing\n",
    "\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "initial_w  = np.zeros((tx_train.shape[1],1))\n",
    "\n",
    "w, rmse = learning_by_newton_method(y_train, tx_train, initial_w, 100, gamma)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "#alpha = 0.001\n",
    "lambd = 0.1\n",
    "ratio = 0.05\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-5, -1, 10)\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_99, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "for gamma in gammas:\n",
    "    initial_w  = np.ones((tx_train.shape[1],1))\n",
    "    w_NM, rmse_NM = learning_by_newton_method(y_train, tx_train, initial_w, max_iter, gamma)\n",
    "#     lossREG = compute_loss(y_test, tx_test, w)\n",
    "#     rmse = np.sqrt(2*lossREG)\n",
    "    losses.append(rmse_NM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(gammas[0:5], losses[0:5], marker=\".\", color='b')\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_gradient_descent you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 20\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "max_iters = 5\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation_laz(y, x_99, k_fold, seed, lambd, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_newton_method you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "#w = np.ones((x_train.shape[1]+1, 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_RegPen, tr_rmse, te_rmse = cross_validation_laz(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenvectors, eigenvalues, V = np.linalg.svd(X_test.T, full_matrices=False)\n",
    "x_proj_test = np.dot(X_test, eigenvectors[:, 0:15])\n",
    "\n",
    "tX_test_proj = np.c_[np.ones((x_proj_test.shape[0], 1)), x_proj_test]\n",
    "\n",
    "weights = w_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test_proj)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
