{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** LS ****\n",
      "0.824319370683\n",
      "0.826956343995\n",
      "\t*** GS ****\n",
      "0.928404908633\n",
      "0.928677901986\n",
      "\t*** N ****\n",
      "0.999809002735\n",
      "0.999810306918\n",
      "\t*** CLS ****\n",
      "-mean weights:\n",
      "0.823861760302\n",
      "0.82732031017\n",
      "-best weights:\n",
      "0.823867919796\n",
      "0.827321865538\n",
      "\t**** Penalized *******\n",
      "0.999808995056\n",
      "0.999810331214\n",
      "ratio of misclassified over total predictions: \n",
      "F1 CLS: [ 0.25511556]\n",
      "F1 GS: [ 0.30983111]\n",
      "F1 N: [ 0.31756444]\n",
      "F1 PR: [ 0.31756]\n"
     ]
    }
   ],
   "source": [
    "#Let's modify the data: replace -999 by -9\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -9)\n",
    "\n",
    "ratio = 0.1\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_99, y, ratio)\n",
    "\n",
    "#tx_train_99, tr_mean_99, tr_std_99 = standardize(x_train)\n",
    "#tx_test_99, te_mean_99, te_std_99 = standardize(x_test)\n",
    "\n",
    "tx_train_99 = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test_99 = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "#Least squares\n",
    "print(\"\\t*** LS ****\")\n",
    "w_LS, rmse_tr = least_squares(y_train, tx_train_99)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, w_LS)\n",
    "print(rmse_te)\n",
    "print(rmse_tr)\n",
    "\n",
    "#Gradient Descent\n",
    "print(\"\\t*** GS ****\")\n",
    "gamma = 0.00001\n",
    "max_iters = 5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_GD, rmse_tr_GD, = least_squares_GD(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_GD = compute_RMSE(y_test, tx_test_99, w_GD)\n",
    "print(rmse_te_GD)\n",
    "print(rmse_tr_GD)\n",
    "\n",
    "#Newton\n",
    "print(\"\\t*** N ****\")\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "lambd = 0.5\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_N, rmse_tr_N = learning_by_newton_method(y_train, tx_train_99, initial_w, max_iters, gamma)\n",
    "rmse_te_N = compute_RMSE(y_test, tx_test_99, w_N)\n",
    "print(rmse_te_N)\n",
    "print(rmse_tr_N)\n",
    "\n",
    "#Mean of 10 folds least squares\n",
    "print(\"\\t*** CLS ****\")\n",
    "k_folds = 10\n",
    "seed = 2\n",
    "w_LS_folds, rmse_tr_CLS, rmse_te_CLS = cross_validation_LS(y, x_99, k_folds, seed)\n",
    "\n",
    "#Mean of weights along folds\n",
    "w_CLS = w_LS_folds.mean(axis=0)\n",
    "w_CLS = np.reshape(w_CLS, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_mean = compute_loss(y_test, tx_test_99, w_CLS)\n",
    "train_mse_CLS_mean = compute_loss(y_train, tx_train_99, w_CLS)\n",
    "rmse_te_CLS_mean = np.sqrt(2*test_mse_CLS_mean)\n",
    "rmse_tr_CLS_mean = np.sqrt(2*train_mse_CLS_mean)\n",
    "print(\"-mean weights:\")\n",
    "print(rmse_te_CLS_mean)\n",
    "print(rmse_tr_CLS_mean)\n",
    "\n",
    "#Best weights in test results\n",
    "w_CLS_best = w_LS_folds[np.argmin(rmse_te_CLS)]\n",
    "w_CLS_best = np.reshape(w_CLS_best, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_best = compute_loss(y_test, tx_test_99, w_CLS_best)\n",
    "train_mse_CLS_best = compute_loss(y_train, tx_train_99, w_CLS_best)\n",
    "rmse_te_CLS_best = np.sqrt(2*test_mse_CLS_best)\n",
    "rmse_tr_CLS_best = np.sqrt(2*train_mse_CLS_best)\n",
    "print(\"-best weights:\")\n",
    "print(rmse_te_CLS_best)\n",
    "print(rmse_tr_CLS_best)\n",
    "\n",
    "#Penalized Regression\n",
    "lambd = 0.5\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "print(\"\\t**** Penalized *******\")\n",
    "initial_w  = np.zeros((tx_train_99.shape[1],1))\n",
    "w_reg, rmse_tr_reg = reg_logistic_regression(y_train, tx_train_99, lambd, initial_w, max_iters, gamma)\n",
    "rmse_te_reg = compute_RMSE(y_test, tx_test_99, w_reg)\n",
    "print(rmse_te_reg)\n",
    "print(rmse_tr_reg)\n",
    "\n",
    "#kind of F1-measure\n",
    "print(\"ratio of misclassified over total predictions: \")\n",
    "\n",
    "y_pred_LS = predict_labels(w_CLS, tx_test_99)\n",
    "f1_CLS = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "print(\"F1 CLS: {l}\".format(l=f1_CLS))\n",
    "\n",
    "y_pred_GS = predict_labels(w_GD, tx_test_99)\n",
    "f1_GS = sum(abs(y_test-y_pred_GS))/(2*len(y_pred_GS))\n",
    "print(\"F1 GS: {l}\".format(l=f1_GS))\n",
    "\n",
    "y_pred_N = predict_labels(w_N, tx_test_99)\n",
    "f1_N = sum(abs(y_test-y_pred_N))/(2*len(y_pred_N))\n",
    "print(\"F1 N: {l}\".format(l=f1_N))\n",
    "\n",
    "y_pred_PR = predict_labels(w_reg, tx_test_99)\n",
    "f1_PR = sum(abs(y_test-y_pred_PR))/(2*len(y_pred_PR))\n",
    "print(\"F1 PR: {l}\".format(l=f1_PR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,k_folds), rmse_tr_CLS)\n",
    "plt.plot(range(0,k_folds), rmse_te_CLS, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot variance and mean of test/train error of Least squares\n",
    "plt.boxplot([rmse_te_CLS, rmse_tr_CLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for LS: [ 0.2193]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'set_xscale'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-7ad7675669bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mf1_std\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_rid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'set_xscale'"
     ]
    }
   ],
   "source": [
    "x_nan = x.copy()\n",
    "np.putmask(x_nan, x_nan==-999, np.nan)\n",
    "x_square = np.zeros((x_nan.shape[0], 3*x_nan.shape[1]))\n",
    "\n",
    "\n",
    "for column in range(0,x_nan.shape[1]):\n",
    "    x_square[:, column] = x_nan[:, column]\n",
    "    x_square[:, column+x_nan.shape[1]] = np.multiply(x_nan[:,column], x_nan[:,column])\n",
    "    x_square[:, column+2*x_nan.shape[1]] = np.multiply(np.multiply(x_nan[:,column], x_nan[:,column]), x_nan[:,column])\n",
    "\n",
    "\n",
    "x_square[np.isnan(x_square)]=-9\n",
    "\n",
    "ratio = 0.4\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_square, y, ratio)\n",
    "\n",
    "#tx_train_square, m, s = standardize(x_train)\n",
    "#tx_test_square, m, s = standardize(x_test)\n",
    "\n",
    "tx_train_square = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test_square = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.ones((tx_train_square.shape[1], 1))\n",
    "\n",
    "w_LS_pow3, rmse_LS_pow3 = least_squares(y_train, tx_train_square)\n",
    "y_pred_LS = predict_labels(w_LS_pow3, tx_test_square)\n",
    "f1_LS_pow3 = sum(abs(y_test-y_pred_LS))/(2*len(y_pred_LS))\n",
    "print(\"f1 score for LS: {f}\".format(f=f1_LS_pow3))\n",
    "\n",
    "lambds = np.logspace(-6, -5, 50)\n",
    "best_f1 = 1000\n",
    "best_lamb = 0\n",
    "k_folds = 5\n",
    "seed = 1\n",
    "f1_mean = []\n",
    "f1_std = []\n",
    "for lamb in lambds:\n",
    "    f1_rid, w_rid = cross_validation_ridge(y_train, tx_train_square, lamb, k_folds, seed)\n",
    "    f1_rid = np.reshape(f1_rid, (len(f1_rid), 1))\n",
    "    f1_mean.append(f1_rid.mean())\n",
    "    f1_std.append(f1_rid.std())\n",
    "\n",
    "#print(\"the best lmabda is {bl} with {f}\".format(bl=best_lamb, f=best_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEECAYAAAAyMaOFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc581277908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
    "ax.errorbar(lambds, f1_mean, yerr = f1_std)\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78004]\n",
      "[ 0.780524]\n"
     ]
    }
   ],
   "source": [
    "w_rid_mean = weights_ridge.mean(axis=0)\n",
    "w_rid_best = weights_ridge[np.argmin(f1_ridge_folds)]\n",
    "\n",
    "tx_total, m, s = standardize(x_square)\n",
    "\n",
    "y_pred_mean = predict_labels(w_rid_mean, tx_total)\n",
    "y_pred_mean = np.reshape(y_pred_mean, (len(y_pred_mean), 1))\n",
    "f1_rid_mean = sum(abs(y-y_pred_mean))/(2*len(y_pred_mean))\n",
    "\n",
    "print(1-f1_rid_mean)\n",
    "\n",
    "y_pred_best = predict_labels(w_rid_best, tx_total)\n",
    "y_pred_best = np.reshape(y_pred_best, (len(y_pred_best), 1))\n",
    "f1_rid_best = sum(abs(y-y_pred_best))/(2*len(y_pred_best))\n",
    "\n",
    "print(1-f1_rid_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IRLS\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -99)\n",
    "#PCA to avoid correlation and singular matrix\n",
    "#eigenvectors, eigenvalues, V = np.linalg.svd(x_99.T, full_matrices=False)\n",
    "#x_proj = np.dot(x_99, eigenvectors[:, 0:10])\n",
    "#y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "ratio = 0.05\n",
    "x_train_IRLS, x_test_IRLS, y_train_IRLS, y_test_IRLS = split_data(x_99, y, ratio)\n",
    "tx_train_IRLS = np.c_[np.ones((y_train_IRLS.shape[0], 1)), x_train_IRLS]\n",
    "tx_test_IRLS= np.c_[np.ones((y_test_IRLS.shape[0], 1)), x_test_IRLS]\n",
    "\n",
    "max_iters = 5\n",
    "print(\"\\t*** IRLS *****\")\n",
    "initial_w  = np.ones((tx_train_IRLS.shape[1],1))\n",
    "w_IRLS, rmse_tr_IRLS = learning_by_IRLS(y_train_IRLS, tx_train_IRLS, initial_w, max_iters)\n",
    "rmse_te_IRLS = compute_RMSE(y_test_IRLS, tx_test_IRLS, w_IRLS)\n",
    "print(rmse_te_IRLS)\n",
    "print(rmse_tr_IRLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Not Sure what you wanted to achieve here\n",
    "mean_w = w_LS.mean(axis=0)\n",
    "std_w = w_LS.std(axis=0)\n",
    "rmse_tr = compute_RMSE(y_train, tx_train_99, mean_w)\n",
    "rmse_te = compute_RMSE(y_test, tx_test_99, mean_w)\n",
    "print(rmse_tr)\n",
    "print(rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_s = standardize(x)\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:12])\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)\n",
    "print(V.shape)\n",
    "print(x_proj.shape)\n",
    "sigma = x_proj.std(axis=0).mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Something is missing\n",
    "\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "initial_w  = np.zeros((tx_train.shape[1],1))\n",
    "\n",
    "w, rmse = learning_by_newton_method(y_train, tx_train, initial_w, 100, gamma)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "#alpha = 0.001\n",
    "lambd = 0.1\n",
    "ratio = 0.05\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-5, -1, 10)\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_99, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "for gamma in gammas:\n",
    "    initial_w  = np.ones((tx_train.shape[1],1))\n",
    "    w_NM, rmse_NM = learning_by_newton_method(y_train, tx_train, initial_w, max_iter, gamma)\n",
    "#     lossREG = compute_loss(y_test, tx_test, w)\n",
    "#     rmse = np.sqrt(2*lossREG)\n",
    "    losses.append(rmse_NM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(gammas[0:5], losses[0:5], marker=\".\", color='b')\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_gradient_descent you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 20\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "max_iters = 5\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation_laz(y, x_99, k_fold, seed, lambd, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was it working? in your version of learning_by_newton_method you had to provide max_iters to the function\n",
    "# just as in my implemetnation\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "#w = np.ones((x_train.shape[1]+1, 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_RegPen, tr_rmse, te_rmse = cross_validation_laz(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "x_nan_test = X_test.copy()\n",
    "np.putmask(x_nan_test, x_nan_test==-999, np.nan)\n",
    "x_third = np.zeros((x_nan_test.shape[0], 3*x_nan_test.shape[1]))\n",
    "\n",
    "for column in range(0,x_nan_test.shape[1]):\n",
    "    x_third[:, column] = x_nan_test[:, column]\n",
    "    x_third[:, column+x_nan_test.shape[1]] = np.multiply(x_nan_test[:,column], x_nan_test[:,column])\n",
    "    x_third[:, column+2*x_nan_test.shape[1]] = np.multiply(np.multiply(x_nan_test[:,column], x_nan_test[:,column]), x_nan_test[:,column])\n",
    "\n",
    "x_third[np.isnan(x_third)]=-9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = w_rid_best\n",
    "tX_test, m, s = standardize(x_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
