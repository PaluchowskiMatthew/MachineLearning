{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "import cProfile\n",
    "from matplotlib.mlab import PCA\n",
    "from implementations import *\n",
    "from costs import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from method_comparison_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Train and Test Data\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pure Numpy version\n",
    "\n",
    "#Lets extract sparse columns which contain -999 values\n",
    "columns = tx.min(axis=0)#tx_train.shape[1]\n",
    "sparse_columns = np.array([])\n",
    "for i, minimum in np.ndenumerate(columns):\n",
    "    if -999 == minimum:\n",
    "        sparse_columns = np.append(sparse_columns, [i])\n",
    "sparse_columns = sparse_columns.astype(int)\n",
    "print('Sparse columns:')        \n",
    "print(sparse_columns)\n",
    "\n",
    "# Lets replace -999 values with nan's\n",
    "tx_nan = tx.copy()\n",
    "tx_nan[tx_nan==-999]=np.nan\n",
    "print(np.nanmean(tx_nan, axis=0))\n",
    "print(np.nanstd(tx_nan, axis=0))\n",
    "print(np.nanmin(tx_nan, axis=0))\n",
    "print(np.nanmax(tx_nan, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PURE NUMPY VERSION\n",
    "\n",
    "#Lets fill NaNs with -99\n",
    "tx_99_filled = tx_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_99_filled)\n",
    "tx_99_filled[where_are_NaNs] = -99\n",
    "print(np.mean(tx_99_filled, axis=0))\n",
    "print(np.std(tx_99_filled, axis=0))\n",
    "print(np.min(tx_99_filled, axis=0))\n",
    "print(np.max(tx_99_filled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXPLORATORY DATASET 1\n",
    "# PURE NUMPY VERSION\n",
    "\n",
    "#Lets fill NaNs with column's mean value\n",
    "tx_mean_filled = tx_nan.copy()\n",
    "#Obtain mean of columns as you need, nanmean is just convenient.\n",
    "mean = np.nanmean(tx_mean_filled, axis=0)\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(np.isnan(tx_mean_filled))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "tx_mean_filled[inds]=np.take(mean,inds[1])\n",
    "#Lets normalize\n",
    "tx_mean_filled_normalized = (tx_mean_filled - tx_mean_filled.mean(axis=0)) / tx_mean_filled.std(axis=0, ddof=1)\n",
    "print(np.mean(tx_mean_filled_normalized, axis=0))\n",
    "print(np.std(tx_mean_filled_normalized, axis=0))\n",
    "print(np.min(tx_mean_filled_normalized, axis=0))\n",
    "print(np.max(tx_mean_filled_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXPLORATORY DATASET 2\n",
    "# PURE NUMPY VERSION\n",
    "\n",
    "#Lets fill NaNs with 0\n",
    "tx_zero_filled = tx_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_zero_filled)\n",
    "tx_zero_filled[where_are_NaNs] = 0\n",
    "#Lets normalize\n",
    "tx_zero_filled_normalized = (tx_zero_filled - tx_zero_filled.mean(axis=0)) / tx_zero_filled.std(axis=0, ddof=1)\n",
    "print(np.mean(tx_zero_filled_normalized, axis=0))\n",
    "print(np.std(tx_zero_filled_normalized, axis=0))\n",
    "print(np.min(tx_zero_filled_normalized, axis=0))\n",
    "print(np.max(tx_zero_filled_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXPLORATORY DATASET 3\n",
    "# PURE NUMPY\n",
    "\n",
    "# Lets sum all sprase columns and combine it into new one\n",
    "tx_sparse_dropped = tx_nan.copy()\n",
    "sparse_sum = np.array([np.nansum(tx_sparse_dropped[:,sparse_columns],axis=1)]).T\n",
    "# print(sparse_sum)\n",
    "# Delete sparse columns\n",
    "tx_sparse_dropped = np.delete(tx_sparse_dropped, sparse_columns, axis=1)\n",
    "tx_sparse_dropped = np.hstack((tx_sparse_dropped, sparse_sum))\n",
    "# Normalize\n",
    "tx_sparse_dropped_normalized = (tx_sparse_dropped - tx_sparse_dropped.mean(axis=0)) / tx_sparse_dropped.std(axis=0, ddof=1)\n",
    "\n",
    "print(np.mean(tx_sparse_dropped_normalized, axis=0))\n",
    "print(np.std(tx_sparse_dropped_normalized, axis=0))\n",
    "print(np.min(tx_sparse_dropped_normalized, axis=0))\n",
    "print(np.max(tx_sparse_dropped_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXPLORATORY 4\n",
    "# PURE NUMPY VERSION\n",
    "\n",
    "# instead of -999 we have -99\n",
    "tx_99_filled = tx_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_99_filled)\n",
    "tx_99_filled[where_are_NaNs] = -99\n",
    "#Lets normalize\n",
    "tx_99_filled_normalized = (tx_99_filled - tx_99_filled.mean(axis=0)) / tx_99_filled.std(axis=0, ddof=1)\n",
    "print(np.mean(tx_99_filled_normalized, axis=0))\n",
    "print(np.std(tx_99_filled_normalized, axis=0))\n",
    "print(np.min(tx_99_filled_normalized, axis=0))\n",
    "print(np.max(tx_99_filled_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools  \n",
    "\n",
    "# Polynomial helper functions\n",
    "\n",
    "def power_of_2_tx(x):\n",
    "    return x*x\n",
    "\n",
    "\n",
    "def combinations(array2d, indeces_list_a, indeces_list_b):\n",
    "    combinations = list(itertools.product(indeces_list_a, indeces_list_b))\n",
    "    for comb in combinations:\n",
    "        new_feature = np.array([array2d[:,comb[0]] * array2d[:,comb[1]]]).T\n",
    "        array2d = np.hstack((array2d, new_feature))\n",
    "    return array2d\n",
    "\n",
    "# arr = np.array([[1,2],[1,2],[1,2]])\n",
    "# print(arr)\n",
    "# combinations(arr, [0], [1])\n",
    "#         numpy.apply_along_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exploratory DATASET 5\n",
    "# normalized -99 filled dataset with base dimensions, \n",
    "# (base dimensions)^2 and permutatons of dimensions which are highly corelated \n",
    "# PURE NUMPY VERSION\n",
    "\n",
    "tx_polynomial = tx_nan.copy()\n",
    "tx_squared = np.apply_along_axis(power_of_2_tx, 1, tx_polynomial)\n",
    "tx_polynomial = np.hstack((tx_polynomial, tx_squared))\n",
    "tx_polynomial_normalized = (tx_polynomial - np.nanmean(tx_polynomial, axis=0)) / np.nanstd(tx_polynomial, axis=0, ddof=1)\n",
    "tx_polynomial_normalized = combinations(tx_polynomial_normalized, [0], [1,2,7,8])\n",
    "tx_polynomial_normalized = combinations(tx_polynomial_normalized, [4], [5,6])\n",
    "tx_polynomial_normalized = combinations(tx_polynomial_normalized, [10], [16])\n",
    "tx_polynomial_normalized = combinations(tx_polynomial_normalized, [21], [23,26,29])\n",
    "tx_polynomial_normalized = combinations(tx_polynomial_normalized, [23], [26,29])\n",
    "where_are_NaNs = np.isnan(tx_polynomial_normalized)\n",
    "tx_polynomial_normalized[where_are_NaNs] = -5 #because np.min(np.nanmin(tx_polynomial_normalized, axis=0)) = -85\n",
    "print(tx_polynomial_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exploratory DATASET 6\n",
    "# binarized categories, normalized -99 filled dataset with base dimensions, \n",
    "# (base dimensions)^2 and permutatons of dimensions which are highly corelated \n",
    "# PURE NUMPY VERSION\n",
    "\n",
    "tx_bin_polynomial = tx_nan.copy()\n",
    "tx_squared = np.apply_along_axis(power_of_2_tx, 1, tx_bin_polynomial)\n",
    "tx_bin_polynomial = np.hstack((tx_bin_polynomial, tx_squared))\n",
    "\n",
    "\n",
    "categories_list = np.unique(tx_bin_polynomial[:,22]).astype(int)\n",
    "categories = np.empty(shape=[tx_bin_polynomial.shape[0], 0])\n",
    "for cat in categories_list:\n",
    "    zeros = np.zeros((tx_bin_polynomial.shape[0], 1))\n",
    "    inds = np.where(tx_bin_polynomial[:,22] == cat)\n",
    "    zeros[inds,0] = 1\n",
    "    categories = np.hstack((categories, zeros))\n",
    "tx_bin_polynomial = np.hstack((categories, tx_bin_polynomial))\n",
    "tx_bin_polynomial = np.delete(tx_bin_polynomial, [22], axis=1)\n",
    "tx_bin_polynomial_normalized = (tx_bin_polynomial - np.nanmean(tx_bin_polynomial, axis=0)) / np.nanstd(tx_bin_polynomial, axis=0, ddof=1)\n",
    "\n",
    "tx_bin_polynomial_normalized = combinations(tx_bin_polynomial_normalized, [0], [1,2,7,8])\n",
    "tx_bin_polynomial_normalized = combinations(tx_bin_polynomial_normalized, [4], [5,6])\n",
    "tx_bin_polynomial_normalized = combinations(tx_bin_polynomial_normalized, [10], [16])\n",
    "tx_bin_polynomial_normalized = combinations(tx_bin_polynomial_normalized, [21], [23,26,29])\n",
    "tx_bin_polynomial_normalized = combinations(tx_bin_polynomial_normalized, [23], [26,29])\n",
    "where_are_NaNs = np.isnan(tx_bin_polynomial_normalized)\n",
    "tx_bin_polynomial_normalized[where_are_NaNs] = -5 #because np.min(np.nanmin(tx_bin_polynomial_normalized, axis=0)) = -85\n",
    "print(tx_bin_polynomial_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets do exactly the same for Predtiction dataset\n",
    "# PURE NUMPY VERSION\n",
    "DATA_PRED_PATH = '../data/test.csv'\n",
    "_, tx_pred, ids_pred = load_csv_data(DATA_PRED_PATH)\n",
    "\n",
    "# Lets replace -999 values with nan's\n",
    "tx_pred_nan = tx_pred.copy()\n",
    "tx_pred_nan[tx_pred_nan==-999]=np.nan\n",
    "\n",
    "# EXPLORATORY DATASET 1\n",
    "#Lets fill NaNs with column's mean value\n",
    "tx_pred_mean_filled = tx_pred_nan.copy()\n",
    "#Obtain mean of columns as you need, nanmean is just convenient.\n",
    "mean = np.nanmean(tx_pred_mean_filled, axis=0)\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(np.isnan(tx_pred_mean_filled))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "tx_pred_mean_filled[inds]=np.take(mean,inds[1])\n",
    "#Lets normalize\n",
    "tx_pred_mean_filled_normalized = (tx_pred_mean_filled - tx_pred_mean_filled.mean(axis=0)) / tx_pred_mean_filled.std(axis=0, ddof=1)\n",
    "\n",
    "\n",
    "# EXPLORATORY DATASET 2\n",
    "#Lets fill NaNs with 0\n",
    "tx_pred_zero_filled = tx_pred_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_pred_zero_filled)\n",
    "tx_pred_zero_filled[where_are_NaNs] = 0\n",
    "#Lets normalize\n",
    "tx_pred_zero_filled_normalized = (tx_pred_zero_filled - tx_pred_zero_filled.mean(axis=0)) / tx_pred_zero_filled.std(axis=0, ddof=1)\n",
    "\n",
    "\n",
    "# EXPLORATORY DATASET 3\n",
    "# Lets sum all sprase columns and combine it into new one\n",
    "tx_pred_sparse_dropped = tx_pred_nan.copy()\n",
    "sparse_sum = np.array([np.nansum(tx_pred_sparse_dropped[:,sparse_columns],axis=1)]).T\n",
    "# print(sparse_sum)\n",
    "# Delete sparse columns\n",
    "tx_pred_sparse_dropped = np.delete(tx_pred_sparse_dropped, sparse_columns, axis=1)\n",
    "tx_pred_sparse_dropped = np.hstack((tx_pred_sparse_dropped, sparse_sum))\n",
    "# Normalize\n",
    "tx_pred_sparse_dropped_normalized = (tx_pred_sparse_dropped - tx_pred_sparse_dropped.mean(axis=0)) / tx_pred_sparse_dropped.std(axis=0, ddof=1)\n",
    "\n",
    "\n",
    "# EXPLORATORY 4\n",
    "# instead of -999 we have -99\n",
    "tx_pred_99_filled = tx_pred_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_pred_99_filled)\n",
    "tx_pred_99_filled[where_are_NaNs] = -99\n",
    "#Lets normalize\n",
    "tx_pred_99_filled_normalized = (tx_pred_99_filled - tx_pred_99_filled.mean(axis=0)) / tx_pred_99_filled.std(axis=0, ddof=1)\n",
    "\n",
    "# Exploratory DATASET 5\n",
    "# normalized -5 filled dataset with base dimensions, \n",
    "# (base dimensions)^2 and permutatons of dimensions which are highly corelated \n",
    "tx_pred_polynomial = tx_pred_nan.copy()\n",
    "tx_pred_squared = np.apply_along_axis(power_of_2_tx, 1, tx_pred_polynomial)\n",
    "tx_pred_polynomial = np.hstack((tx_pred_polynomial, tx_pred_squared))\n",
    "tx_pred_polynomial_normalized = (tx_pred_polynomial - np.nanmean(tx_pred_polynomial, axis=0)) / np.nanstd(tx_pred_polynomial, axis=0, ddof=1)\n",
    "tx_pred_polynomial_normalized = combinations(tx_pred_polynomial_normalized, [0], [1,2,7,8])\n",
    "tx_pred_polynomial_normalized = combinations(tx_pred_polynomial_normalized, [4], [5,6])\n",
    "tx_pred_polynomial_normalized = combinations(tx_pred_polynomial_normalized, [10], [16])\n",
    "tx_pred_polynomial_normalized = combinations(tx_pred_polynomial_normalized, [21], [23,26,29])\n",
    "tx_pred_polynomial_normalized = combinations(tx_pred_polynomial_normalized, [23], [26,29])\n",
    "where_are_NaNs = np.isnan(tx_pred_polynomial_normalized)\n",
    "tx_pred_polynomial_normalized[where_are_NaNs] = -5\n",
    "\n",
    "\n",
    "# Exploratory DATASET 6\n",
    "# binarized categories, normalized -99 filled dataset with base dimensions, \n",
    "# (base dimensions)^2 and permutatons of dimensions which are highly corelated \n",
    "tx_pred_bin_polynomial = tx_pred_nan.copy()\n",
    "tx_pred_squared = np.apply_along_axis(power_of_2_tx, 1, tx_pred_bin_polynomial)\n",
    "tx_pred_bin_polynomial = np.hstack((tx_pred_bin_polynomial, tx_pred_squared))\n",
    "\n",
    "categories_list = np.unique(tx_pred_bin_polynomial[:,22]).astype(int)\n",
    "categories = np.empty(shape=[tx_pred_bin_polynomial.shape[0], 0])\n",
    "for cat in categories_list:\n",
    "    zeros = np.zeros((tx_pred_bin_polynomial.shape[0], 1))\n",
    "    inds = np.where(tx_pred_bin_polynomial[:,22] == cat)\n",
    "    zeros[inds,0] = 1\n",
    "    categories = np.hstack((categories, zeros))\n",
    "tx_pred_bin_polynomial = np.hstack((categories, tx_pred_bin_polynomial))\n",
    "tx_pred_bin_polynomial = np.delete(tx_pred_bin_polynomial, [22], axis=1)\n",
    "tx_pred_bin_polynomial_normalized = (tx_pred_bin_polynomial - np.nanmean(tx_pred_bin_polynomial, axis=0)) / np.nanstd(tx_pred_bin_polynomial, axis=0, ddof=1)\n",
    "\n",
    "\n",
    "tx_pred_bin_polynomial_normalized = combinations(tx_pred_bin_polynomial_normalized, [0], [1,2,7,8])\n",
    "tx_pred_bin_polynomial_normalized = combinations(tx_pred_bin_polynomial_normalized, [4], [5,6])\n",
    "tx_pred_bin_polynomial_normalized = combinations(tx_pred_bin_polynomial_normalized, [10], [16])\n",
    "tx_pred_bin_polynomial_normalized = combinations(tx_pred_bin_polynomial_normalized, [21], [23,26,29])\n",
    "tx_pred_bin_polynomial_normalized = combinations(tx_pred_bin_polynomial_normalized, [23], [26,29])\n",
    "where_are_NaNs = np.isnan(tx_pred_bin_polynomial_normalized)\n",
    "tx_pred_bin_polynomial_normalized[where_are_NaNs] = -5 #because np.min(np.nanmin(tx_pred_bin_polynomial_normalized, axis=0)) = -85\n",
    "print(tx_pred_bin_polynomial_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets split tx to train and test\n",
    "split_ratio = 0.2\n",
    "tx_train, tx_test, y_train, y_test = split_data(tx, y, split_ratio)\n",
    "tx_zero_filled_normalized_train, tx_zero_filled_normalized_test, y_zero_filled_normalized_train, y_zero_filled_normalized_test = split_data(tx_zero_filled_normalized, y, split_ratio)\n",
    "tx_mean_filled_normalized_train, tx_mean_filled_normalized_test, y_mean_filled_normalized_train, y_mean_filled_normalized_test = split_data(tx_mean_filled_normalized, y, split_ratio)\n",
    "tx_sparse_dropped_normalized_train, tx_sparse_dropped_normalized_test, y_sparse_dropped_normalized_train, y_sparse_dropped_normalized_test = split_data(tx_sparse_dropped_normalized, y, split_ratio)\n",
    "tx_99_filled_normalized_train, tx_99_filled_normalized_test, y_99_filled_normalized_train, y_99_filled_normalized_test = split_data(tx_99_filled_normalized, y, split_ratio)\n",
    "tx_polynomial_normalized_train, tx_polynomial_normalized_test, y_polynomial_normalized_train, y_polynomial_normalized_test = split_data(tx_polynomial_normalized, y, split_ratio)\n",
    "tx_bin_polynomial_normalized_train, tx_bin_polynomial_normalized_test, y_bin_polynomial_normalized_train, y_bin_polynomial_normalized_test = split_data(tx_bin_polynomial_normalized, y, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of datasets\n",
    "\n",
    "# Test datasets\n",
    "train_datasets = [tx_train, tx_zero_filled_normalized_train, tx_mean_filled_normalized_train, tx_sparse_dropped_normalized_train, tx_99_filled_normalized_train, tx_polynomial_normalized_train, tx_bin_polynomial_normalized_train]\n",
    "new_train_datasets = [tx_zero_filled_normalized_train, tx_mean_filled_normalized_train, tx_sparse_dropped_normalized_train, tx_99_filled_normalized_train, tx_polynomial_normalized_train, tx_bin_polynomial_normalized_train]\n",
    "\n",
    "# Test datasets\n",
    "test_datasets = [tx_test, tx_zero_filled_normalized_test, tx_mean_filled_normalized_test, tx_sparse_dropped_normalized_test, tx_99_filled_normalized_test, tx_polynomial_normalized_test, tx_bin_polynomial_normalized_test]\n",
    "new_test_datasets = [tx_zero_filled_normalized_test, tx_mean_filled_normalized_test, tx_sparse_dropped_normalized_test, tx_99_filled_normalized_test, tx_polynomial_normalized_test, tx_bin_polynomial_normalized_test]\n",
    "\n",
    "# Prediction datasets\n",
    "pred_datasets = [tx_pred, tx_pred_mean_filled_normalized, tx_pred_zero_filled_normalized, tx_pred_sparse_dropped_normalized, tx_pred_99_filled_normalized, tx_pred_polynomial_normalized, tx_pred_bin_polynomial_normalized]\n",
    "new_pred_datasets = [tx_pred_mean_filled_normalized, tx_pred_zero_filled_normalized, tx_pred_sparse_dropped_normalized, tx_pred_99_filled_normalized, tx_pred_polynomial_normalized, tx_pred_bin_polynomial_normalized]\n",
    "\n",
    "datasets_names = ['Original/Raw', 'Zero filled', 'Mean filled', 'NaN dropped', '-99 filled', 'Polynomial', 'Binary Category Polynomial']\n",
    "new_datasets_names = ['Zero filled', 'Mean filled', 'NaN dropped', '-99 filled', 'Polynomial', 'Binary Category Polynomial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters \n",
    "max_iters = 5000\n",
    "gammas = np.linspace(1.6e-12, 0.8e-11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, weight = logistic_regression_dataset_gammas_test(y_polynomial_normalized_train,\n",
    "                                                              y_polynomial_normalized_test,\n",
    "                                                              tx_polynomial_normalized_train, \n",
    "                                                              tx_polynomial_normalized_test,\n",
    "                                                              max_iters,\n",
    "                                                              gammas, \n",
    "                                                              'Polynomial',\n",
    "                                                              1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.8e-11\n",
    "max_iters = 1000\n",
    "logistic_regression_dataset_single_gamma_test(y_polynomial_normalized_train,\n",
    "                                              y_polynomial_normalized_test,\n",
    "                                              tx_polynomial_normalized_train,\n",
    "                                              tx_polynomial_normalized_test,\n",
    "                                              max_iters,\n",
    "                                              gamma,\n",
    "                                              'Squared Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression of all datasets vs gammas. \n",
    "# WARNING Takes a lot of time for all datasets (~30 min)\n",
    "\n",
    "#Parameters \n",
    "max_iters = 100\n",
    "gammas = np.logspace(-12, -11, 2)\n",
    "\n",
    "for i in range(len(new_train_datasets)):\n",
    "    # Parameters\n",
    "    train_dataset = new_train_datasets[i]\n",
    "    test_dataset = new_test_datasets[i]\n",
    "    dataset_name = new_datasets_names[i]\n",
    "    figure_id = i\n",
    "    \n",
    "    logistic_regression_dataset_gammas_test(y_train,\n",
    "                                            y_test,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            max_iters,\n",
    "                                            gammas,\n",
    "                                            dataset_name,\n",
    "                                            figure_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_iters = 100\n",
    "gamma = 1e-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "weights = []\n",
    "for i in range(len(new_train_datasets)):\n",
    "    train_dataset = new_train_datasets[i]\n",
    "    test_dataset = new_test_datasets[i]\n",
    "    dataset_name = new_datasets_names[i]\n",
    "        \n",
    "    train_rmse, test_rmse, weight = logistic_regression_dataset_single_gamma_test(y_train,\n",
    "                                                                                  y_test,\n",
    "                                                                                  train_dataset,\n",
    "                                                                                  test_dataset,\n",
    "                                                                                  max_iters,\n",
    "                                                                                  gamma,\n",
    "                                                                                  dataset_name)\n",
    "    weights.append(weight)\n",
    "    train_losses = np.append(train_losses, train_rmse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Losses of Test Datasets\")\n",
    "plt.plot(range(len(new_train_datasets)), test_losses, marker=\".\", color='r', label='test error')\n",
    "plt.xlabel(\"Datasets\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.figure(2)\n",
    "plt.title(\"Losses of Train Datasets\")\n",
    "plt.plot(range(len(new_train_datasets)), train_losses, marker=\".\", color='b', label='train error')\n",
    "plt.xlabel(\"Datasets\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters= 100\n",
    "gamma = 1.2e-3\n",
    "initial_w = np.zeros(tx_polynomial_normalized_train.shape[1])\n",
    "gradient_w, train_rmse = least_squares_GD(y_polynomial_normalized_train,\n",
    "                                          tx_polynomial_normalized_train,\n",
    "                                          initial_w,\n",
    "                                          max_iters,\n",
    "                                          gamma)\n",
    "test_rmse = compute_RMSE(y_polynomial_normalized_test, tx_polynomial_normalized_test, gradient_w)\n",
    "print(train_rmse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gammas = np.linspace(1e-3, 0.4e-2, 10)\n",
    "for i in range(len(new_train_datasets)):\n",
    "    # Parameters\n",
    "    train_dataset = new_train_datasets[i]\n",
    "    test_dataset = new_test_datasets[i]\n",
    "    dataset_name = new_datasets_names[i]\n",
    "    figure_id = i\n",
    "    \n",
    "    least_squares_GD_gammas_test(y_train, y_test, train_dataset, test_dataset, gammas, max_iters, dataset_name, figure_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "weights = []\n",
    "for i in range(len(new_train_datasets)):\n",
    "    train_dataset = new_train_datasets[i]\n",
    "    test_dataset = new_test_datasets[i]\n",
    "    dataset_name = new_datasets_names[i]\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    least_squares_w, train_rmse  = least_squares(y_train, train_dataset)\n",
    "    weights.append(least_squares_w)\n",
    "\n",
    "    test_rmse = compute_RMSE(y_test, test_dataset, least_squares_w)\n",
    "    \n",
    "    train_losses = np.append(train_losses, train_rmse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Lest Squares: execution time={t:.3f} seconds. RMSE Train Loss={l}, Test Loss={tl}\".format(t=exection_time, l=train_rmse, tl=test_rmse))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Losses of Test Datasets\")\n",
    "plt.plot(range(len(new_train_datasets)), test_losses, marker=\".\", color='r', label='test error')\n",
    "plt.xlabel(\"Datasets\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.figure(2)\n",
    "plt.title(\"Losses of Train Datasets\")\n",
    "plt.plot(range(len(new_train_datasets)), train_losses, marker=\".\", color='b', label='train error')\n",
    "plt.xlabel(\"Datasets\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "least_squares_w, train_rmse = least_squares(y_bin_polynomial_normalized_train, tx_bin_polynomial_normalized_train)\n",
    "\n",
    "test_rmse = compute_RMSE(y_bin_polynomial_normalized_test, tx_bin_polynomial_normalized_test, least_squares_w)\n",
    "print(train_rmse, test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-10, -1, 100)\n",
    "for i in range(len(new_train_datasets)):\n",
    "    # Parameters\n",
    "    train_dataset = new_train_datasets[i]\n",
    "    test_dataset = new_test_datasets[i]\n",
    "    dataset_name = new_datasets_names[i]\n",
    "    figure_id = i\n",
    "    \n",
    "    ridge_regression_dataset_lamdas_test(y_train, y_test, train_dataset, test_dataset, lambdas, dataset_name, figure_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lamb = 0.9e-3\n",
    "ridge_regression_gradient_w, train_rmse = ridge_regression(y_polynomial_normalized_train,\n",
    "                                                           tx_polynomial_normalized_train,\n",
    "                                                           lamb)\n",
    "test_rmse = compute_RMSE(y_polynomial_normalized_test,\n",
    "                         tx_polynomial_normalized_test,\n",
    "                         ridge_regression_gradient_w)\n",
    "print(train_rmse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "for i in range(len(new_train_datasets)):\n",
    "    train_dataset = new_train_datasets[i]\n",
    "    test_dataset = new_test_datasets[i]\n",
    "    dataset_name = new_datasets_names[i]\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    ridge_regression_gradient_w, ridge_regression_loss,  = ridge_regression(y_train, train_dataset, lamb)\n",
    "\n",
    "    train_losses = np.append(train_losses, ridge_regression_loss)\n",
    "\n",
    "    test_RMSe = compute_RMSE(y_test, test_dataset, ridge_regression_gradient_w)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Ridge Regression for {dn}: execution time={t:.3f} seconds. Test RMSE Loss={l}, Train RMSE Loss={tl}\".format(dn = dataset_name, t=exection_time, l=test_rmse, tl=ridge_regression_loss))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Losses of Test Datasets\")\n",
    "plt.plot(range(len(new_train_datasets)), test_losses, marker=\".\", color='r', label='test error')\n",
    "plt.xlabel(\"Datasets\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.figure(2)\n",
    "plt.title(\"Losses of Train Datasets\")\n",
    "plt.plot(range(len(new_train_datasets)), train_losses, marker=\".\", color='b', label='train error')\n",
    "plt.xlabel(\"Datasets\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "subset_y = y\n",
    "subset_tx = tx_polynomial_normalized\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "seed = 1\n",
    "k_fold = 10\n",
    "lambdas = np.logspace(-16, 2, 1)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "# weights = np.empty((0,subset_tx.shape[1]), float)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for lambd in np.nditer(lambdas):\n",
    "    loss_tr, loss_te = cross_validation_mat(subset_y, subset_tx, k_fold, seed, lambd)\n",
    "    rmse_tr = np.append(rmse_tr, loss_tr)\n",
    "    rmse_te = np.append(rmse_te, loss_te)\n",
    "#     weights = np.vstack((weights, w))\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"Cross Validation: execution time={t:.3f} seconds.\".format(t=exection_time))\n",
    "#cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/least_squares_polynomial_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "weights_pred = least_squares_w #for tx_polynomial_normalized_train\n",
    "y_pred = predict_labels(weights_pred, tx_pred_bin_polynomial_normalized)\n",
    "create_csv_submission(ids_pred, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
