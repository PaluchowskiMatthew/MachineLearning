{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "import cProfile\n",
    "from implementations import *\n",
    "from costs import *\n",
    "from method_comparison_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "# Load train data\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#Lets verify loaded data\n",
    "print(y.shape)\n",
    "print(tx.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 30) (50000,)\n",
      "(200000, 30) (200000,)\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 0.2\n",
    "tx_train, tx_test, y_train, y_test = split_data(tx, y, split_ratio)\n",
    "print(tx_train.shape, y_train.shape)\n",
    "print(tx_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ones = np.ones((tx.shape[0],1))\n",
    "tx_with_ones = np.hstack((ones, tx))\n",
    "tx_with_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx_with_ones_train, tx_with_ones_test, y_with_ones_train, y_with_ones_test = split_data(tx_with_ones, y, split_ratio)\n",
    "print(tx_with_ones_train.shape, y_with_ones_train.shape)\n",
    "print(tx_with_ones_test.shape, y_with_ones_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Criteria:\n",
    "1. Competitive Part **(counts one third)**. The final rank of your team in the (private) leaderboard will be translated linearly to a scale from 4 to 6.\n",
    "2. Code **(counts one third)**. In Python. No external libraries allowed! For this first project, we want you to implement and use the methods we have seen in class. The code will be graded by two TAs independently, according to the criteria described:\n",
    "* Rules for the code part:\n",
    "  * Reproducibility: In your submission, you must provide a script run.py which produces exactly the same .csv predictions which you used in your best submission to the competition on Kaggle.\n",
    "  * Documentation: Your ML system must be clearly described in your PDF report and also well- documented in the code itself. A clear ReadMe file must be provided. The documentation must also include all data preparation, feature generation as well as cross-validation steps that you have used.\n",
    "  * In addition to your customized system, don’t forget that your code submission must still also include the 6 basic method implementations as described above in step 2.\n",
    "  * No use of external ML libraries is allowed in Project 1. (It will be allowed in Project 2).\n",
    "  * No external datasets allowed.\n",
    "3. Written Report **(counts one third)**. You will write a maximum 2 page PDF report on your findings, using LaTeX. The code will be graded by two TAs independently, and we will provide you feedback. The main criteria will be if you were able to correctly use, implement and describe the 6 baseline methods mentioned in Step 2 above. This counts half for the written report. In addition, we will grade you on the scientific contribution you made additionally, to improve your predictions. For this part, the criteria are\n",
    "  * scientific novelty\n",
    "  * creativity\n",
    "  * reproducibility\n",
    "  * solid comparison baselines supporting your claims – writeup quality\n",
    "  \n",
    "\n",
    "As usual, your code and report will be automatically checked for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo's\n",
    "\n",
    "* Exploratory data analysis with comments\n",
    "* Dataset cleaning\n",
    "* Comment code and this notebook\n",
    "* Improve predictions to be number one in the keggle!\n",
    "  * construct better features (optional)\n",
    "  * implement additional modifications of basic methods implemented (optional)\n",
    "  * clean and preprocess data\n",
    "* LateX pdf report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-10, -7, 10)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for gamma in np.nditer(gammas):\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    initial_w = np.zeros(tx_with_ones_train.shape[1])\n",
    "    gradient_w, train_rmse = least_squares_GD(y_with_ones_train, tx_with_ones_train, initial_w, max_iters, gamma)\n",
    "\n",
    "    # Print result\n",
    "    test_mse = compute_loss(y_with_ones_test, tx_with_ones_test, gradient_w)\n",
    "    test_rmse = np.sqrt(2*test_mse)    \n",
    "    \n",
    "    train_losses = np.append(train_losses, train_rmse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    #print(\"Gradient Descent: execution time={t:.3f} seconds. Train RMSE Loss={l}, Test RMSE Loss={tl}\".format(t=exection_time, l=grad_loss, tl=test_rmse))\n",
    "    \n",
    "plt.semilogx(gammas, train_losses, marker=\".\", color='b', label='Train')\n",
    "plt.semilogx(gammas, test_losses, marker=\".\", color='r', label='Test')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gammas = np.logspace(-3, -10, 2)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for gamma in np.nditer(gammas):\n",
    "    # Start stochastic gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    initial_w = np.zeros(tx_with_ones_train.shape[1])\n",
    "    stoch_gradient_w, train_rmse = least_squares_SGD(y_with_ones_train, tx_with_ones_train, initial_w, max_iters, gamma)\n",
    "\n",
    "    test_mse = compute_loss(y_with_ones_test, tx_with_ones_test, stoch_gradient_w)\n",
    "    test_rmse = np.sqrt(2*test_mse)    \n",
    "    \n",
    "    train_losses = np.append(train_losses, train_rmse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Stochastic Gradient Descent: execution time={t:.3f} seconds. Train RMSE={l}, Test RMSE={tl}\".format(t=exection_time, l=train_rmse, tl=test_rmse))\n",
    "\n",
    "plt.semilogx(gammas, train_losses, marker=\".\", color='b', label='Train')\n",
    "plt.semilogx(gammas, test_losses, marker=\".\", color='r', label='Test')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least Squares - produce our best keggle result 57th position Mateusz Paluchowski0.74463\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "least_squares_w, least_squares_loss = least_squares(y_with_ones_train, tx_with_ones_train)\n",
    "test_mse = compute_loss(y_with_ones_test, tx_with_ones_test, least_squares_w)\n",
    "test_rmse = np.sqrt(2*test_mse)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Lest Squares: execution time={t:.3f} seconds. RMSE Train Loss={l}, Test Loss={tl}\".format(t=exection_time, l=least_squares_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ridge Regression\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lambs = np.logspace(-5, 8, 100)\n",
    "start_time = datetime.datetime.now()\n",
    "for lamb in np.nditer(lambs):\n",
    "    ridge_regression_gradient_w,  ridge_regression_loss = ridge_regression(y_with_ones_train, tx_with_ones_train, lamb)\n",
    "    \n",
    "    train_losses = np.append(train_losses, ridge_regression_loss)\n",
    "    \n",
    "    test_mse = compute_loss(y_with_ones_test, tx_with_ones_test, ridge_regression_gradient_w)\n",
    "    test_rmse = np.sqrt(2*test_mse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"Ridge Regression: execution time={t:.3f} seconds.\".format(t=exection_time))\n",
    "plt.semilogx(lambs, train_losses, marker=\".\", color='b', label='Train')\n",
    "plt.semilogx(lambs, test_losses, marker=\".\", color='r', label='Test')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "#Train RMSE = 0.824233562872 Test RMSE = 1.39476527452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression using gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "weights = np.empty((0,tx_with_ones_train.shape[1]), float)\n",
    "gammas = np.logspace(-18, -23, 10)# np.logspace(-16, -20, 10)\n",
    "for gamma in np.nditer(gammas):\n",
    "    start_time = datetime.datetime.now()\n",
    "    initial_w = np.zeros((tx_with_ones_train.shape[1],1))\n",
    "    logistic_regression_w, logistic_regression_loss = logistic_regression(np.array([y_with_ones_train]).T, tx_with_ones_train, initial_w, max_iters, gamma)\n",
    "    weights = np.vstack((weights, logistic_regression_w.T))\n",
    "\n",
    "    train_losses = np.append(train_losses, logistic_regression_loss)\n",
    "    \n",
    "    test_rmse = compute_RMSE(np.array([y_with_ones_test]).T, tx_with_ones_test, logistic_regression_w)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic Regression: execution time={t:.3f} seconds. Train RMSE={l}, Test RMSE={tl}\".format(t=exection_time, l=logistic_regression_loss, tl=test_rmse))    \n",
    "\n",
    "plt.semilogx(gammas, train_losses, marker=\".\", color='b', label='Train')\n",
    "plt.semilogx(gammas, test_losses, marker=\".\", color='r', label='Test')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression using gradient descent\n",
    "# Slow\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 3.41379310345e-14\n",
    "lambd = 0.1\n",
    "    \n",
    "start_time = datetime.datetime.now()\n",
    "initial_w = np.zeros((tx_with_ones_train.shape[1],1))\n",
    "logistic_regression_w, logistic_regression_loss = reg_logistic_regression(np.array([y_with_ones_train]).T, tx_with_ones_train, lambd, initial_w, max_iters, gamma)\n",
    "test_rmse = compute_RMSE(np.array([y_with_ones_test]).T, tx_with_ones_test, logistic_regression_w)\n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Penalized Logistic Regression: execution time={t:.3f} seconds. Train RMSE={l}, Test RMSE={tl}\".format(t=exection_time, l=logistic_regression_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression using newtons method\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "\n",
    "gammas = np.logspace(-18, -23, 1)\n",
    "for gamma in np.nditer(gammas):\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    initial_w = np.zeros((tx_with_ones_train.shape[1],1))\n",
    "    logistic_regression_newton_w, logistic_regression_newton_loss = learning_by_newton_method(np.array([y_with_ones_train]).T, tx_with_ones_train, initial_w, max_iters, gamma)\n",
    "    test_rmse = compute_RMSE(np.array([y_with_ones_test]).T, tx_with_ones_test, logistic_regression_newton_w)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic Regression Newtons Method: execution time={t:.3f} seconds. Train RMSE={l}, Test RMSE={tl}\".format(t=exection_time, l=logistic_regression_newton_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iteratively reweighted least squares \n",
    "# slow for large dataset, can have matrix singularity problems if many iters are run.\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "initial_w = np.zeros((tx_with_ones_train.shape[1],1))\n",
    "IRLS_w, IRLS_loss  = learning_by_IRLS(np.array([y_with_ones_train[0:10000]]).T, tx_with_ones_train[0:10000], initial_w, max_iters)\n",
    "test_rmse = compute_RMSE(np.array([y_with_ones_test[0:10000]]).T, tx_with_ones_test[0:10000], IRLS_w)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"IRLS: execution time={t:.3f} seconds. Train RMSE={l}, Test RMSE={tl}\".format(t=exection_time, l=IRLS_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.90230794e+01   4.92398193e+01   8.11819816e+01   5.78959617e+01\n",
      "  -7.08420675e+02  -6.01237051e+02  -7.09356603e+02   2.37309984e+00\n",
      "   1.89173324e+01   1.58432217e+02   1.43760943e+00  -1.28304708e-01\n",
      "  -7.08985189e+02   3.87074191e+01  -1.09730480e-02  -8.17107200e-03\n",
      "   4.66602072e+01  -1.95074680e-02   4.35429640e-02   4.17172345e+01\n",
      "  -1.01191920e-02   2.09797178e+02   9.79176000e-01  -3.48329567e+02\n",
      "  -3.99254314e+02  -3.99259788e+02  -6.92381204e+02  -7.09121609e+02\n",
      "  -7.09118631e+02   7.30645914e+01]\n",
      "[ 406.34483401   35.34481492   40.82860887   63.65555431  454.47965615\n",
      "  657.97098617  453.01897051    0.78290955   22.2734492   115.70588372\n",
      "    0.84474126    1.19358245  453.59581401   22.41203584    1.21407622\n",
      "    1.81675941   22.06487828    1.26497962    1.81660763   32.8946274\n",
      "    1.81221908  126.49925272    0.97742435  532.96172343  489.33730734\n",
      "  489.33290465  479.87453609  453.38371728  453.3881105    98.01546598]\n",
      "[ -9.99000000e+02   0.00000000e+00   6.32900000e+00   0.00000000e+00\n",
      "  -9.99000000e+02  -9.99000000e+02  -9.99000000e+02   2.08000000e-01\n",
      "   0.00000000e+00   4.61040000e+01   4.70000000e-02  -1.41400000e+00\n",
      "  -9.99000000e+02   2.00000000e+01  -2.49900000e+00  -3.14200000e+00\n",
      "   2.60000000e+01  -2.50500000e+00  -3.14200000e+00   1.09000000e-01\n",
      "  -3.14200000e+00   1.36780000e+01   0.00000000e+00  -9.99000000e+02\n",
      "  -9.99000000e+02  -9.99000000e+02  -9.99000000e+02  -9.99000000e+02\n",
      "  -9.99000000e+02   0.00000000e+00]\n",
      "[  1.19202600e+03   6.90075000e+02   1.34935100e+03   2.83499900e+03\n",
      "   8.50300000e+00   4.97497900e+03   1.66900000e+01   5.68400000e+00\n",
      "   2.83499900e+03   1.85246200e+03   1.97730000e+01   1.41400000e+00\n",
      "   1.00000000e+00   7.64408000e+02   2.49700000e+00   3.14200000e+00\n",
      "   5.60271000e+02   2.50300000e+00   3.14200000e+00   2.84261700e+03\n",
      "   3.14200000e+00   2.00397600e+03   3.00000000e+00   1.12057300e+03\n",
      "   4.49900000e+00   3.14100000e+00   7.21456000e+02   4.50000000e+00\n",
      "   3.14200000e+00   1.63343300e+03]\n"
     ]
    }
   ],
   "source": [
    "# NUMPY ONLY VERSION\n",
    "\n",
    "#Lets print some basic statistics about dataset\n",
    "print(np.mean(tx, axis=0))\n",
    "print(np.std(tx, axis=0))\n",
    "print(np.min(tx, axis=0))\n",
    "print(np.max(tx, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse columns:\n",
      "[  0.   4.   5.   6.  12.  23.  24.  25.  26.  27.  28.]\n"
     ]
    }
   ],
   "source": [
    "# NUMPY ONLY VERSION\n",
    "\n",
    "#Lets extract sparse columns which contain -999 values\n",
    "columns = tx.min(axis=0)#tx_train.shape[1]\n",
    "sparse_columns = np.array([])\n",
    "for i, minimum in np.ndenumerate(columns):\n",
    "    if -999 == minimum:\n",
    "        sparse_columns = np.append(sparse_columns, [i])\n",
    "print('Sparse columns:')        \n",
    "print(sparse_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.21858528e+02   4.92398193e+01   8.11819816e+01   5.78959617e+01\n",
      "   2.40373503e+00   3.71783360e+02  -8.21688171e-01   2.37309984e+00\n",
      "   1.89173324e+01   1.58432217e+02   1.43760943e+00  -1.28304708e-01\n",
      "   4.58289801e-01   3.87074191e+01  -1.09730480e-02  -8.17107200e-03\n",
      "   4.66602072e+01  -1.95074680e-02   4.35429640e-02   4.17172345e+01\n",
      "  -1.01191920e-02   2.09797178e+02   9.79176000e-01   8.48221045e+01\n",
      "  -3.27458741e-03  -1.23928255e-02   5.76794744e+01  -1.18452642e-02\n",
      "  -1.58228913e-03   7.30645914e+01]\n",
      "[  57.29802145   35.34481492   40.82860887   63.65555431    1.74221431\n",
      "  397.69658434    3.58433731    0.78290955   22.2734492   115.70588372\n",
      "    0.84474126    1.19358245    0.39867861   22.41203584    1.21407622\n",
      "    1.81675941   22.06487828    1.26497962    1.81660763   32.8946274\n",
      "    1.81221908  126.49925272    0.97742435   60.66207397    1.78454002\n",
      "    1.81337943   31.98556122    2.0317286     1.8169372    98.01546598]\n",
      "[  9.044   0.      6.329   0.      0.     13.602 -18.066   0.208   0.\n",
      "  46.104   0.047  -1.414   0.     20.     -2.499  -3.142  26.     -2.505\n",
      "  -3.142   0.109  -3.142  13.678   0.     30.     -4.499  -3.142  30.     -4.5\n",
      "  -3.142   0.   ]\n",
      "[  1.19202600e+03   6.90075000e+02   1.34935100e+03   2.83499900e+03\n",
      "   8.50300000e+00   4.97497900e+03   1.66900000e+01   5.68400000e+00\n",
      "   2.83499900e+03   1.85246200e+03   1.97730000e+01   1.41400000e+00\n",
      "   1.00000000e+00   7.64408000e+02   2.49700000e+00   3.14200000e+00\n",
      "   5.60271000e+02   2.50300000e+00   3.14200000e+00   2.84261700e+03\n",
      "   3.14200000e+00   2.00397600e+03   3.00000000e+00   1.12057300e+03\n",
      "   4.49900000e+00   3.14100000e+00   7.21456000e+02   4.50000000e+00\n",
      "   3.14200000e+00   1.63343300e+03]\n"
     ]
    }
   ],
   "source": [
    "# NUMPY ONLY VERSION\n",
    "\n",
    "# Lets replace -999 values with nan's\n",
    "tx_nan = tx.copy()\n",
    "tx_nan[tx_nan==-999]=np.nan\n",
    "print(np.nanmean(tx_nan, axis=0))\n",
    "print(np.nanstd(tx_nan, axis=0))\n",
    "print(np.nanmin(tx_nan, axis=0))\n",
    "print(np.nanmax(tx_nan, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.21858528e+02   4.92398193e+01   8.11819816e+01   5.78959617e+01\n",
      "   2.40373503e+00   3.71783360e+02  -8.21688171e-01   2.37309984e+00\n",
      "   1.89173324e+01   1.58432217e+02   1.43760943e+00  -1.28304708e-01\n",
      "   4.58289801e-01   3.87074191e+01  -1.09730480e-02  -8.17107200e-03\n",
      "   4.66602072e+01  -1.95074680e-02   4.35429640e-02   4.17172345e+01\n",
      "  -1.01191920e-02   2.09797178e+02   9.79176000e-01   8.48221045e+01\n",
      "  -3.27458741e-03  -1.23928255e-02   5.76794744e+01  -1.18452642e-02\n",
      "  -1.58228913e-03   7.30645914e+01]\n",
      "[  52.74979213   35.34481492   40.82860887   63.65555431    0.9384893\n",
      "  214.22966692    1.93079704    0.78290955   22.2734492   115.70588372\n",
      "    0.84474126    1.19358245    0.21475866   22.41203584    1.21407622\n",
      "    1.81675941   22.06487828    1.26497962    1.81660763   32.8946274\n",
      "    1.81221908  126.49925272    0.97742435   47.00226517    1.38269956\n",
      "    1.40504495   17.22985913    1.09444375    0.97874075   98.01546598]\n",
      "[  9.044   0.      6.329   0.      0.     13.602 -18.066   0.208   0.\n",
      "  46.104   0.047  -1.414   0.     20.     -2.499  -3.142  26.     -2.505\n",
      "  -3.142   0.109  -3.142  13.678   0.     30.     -4.499  -3.142  30.     -4.5\n",
      "  -3.142   0.   ]\n",
      "[  1.19202600e+03   6.90075000e+02   1.34935100e+03   2.83499900e+03\n",
      "   8.50300000e+00   4.97497900e+03   1.66900000e+01   5.68400000e+00\n",
      "   2.83499900e+03   1.85246200e+03   1.97730000e+01   1.41400000e+00\n",
      "   1.00000000e+00   7.64408000e+02   2.49700000e+00   3.14200000e+00\n",
      "   5.60271000e+02   2.50300000e+00   3.14200000e+00   2.84261700e+03\n",
      "   3.14200000e+00   2.00397600e+03   3.00000000e+00   1.12057300e+03\n",
      "   4.49900000e+00   3.14100000e+00   7.21456000e+02   4.50000000e+00\n",
      "   3.14200000e+00   1.63343300e+03]\n"
     ]
    }
   ],
   "source": [
    "# NUMPY ONLY VERSION\n",
    "\n",
    "#Lets fill NaNs with column's mean value\n",
    "tx_mean_filled = tx_nan.copy()\n",
    "#Obtain mean of columns as you need, nanmean is just convenient.\n",
    "mean = np.nanmean(tx_mean_filled, axis=0)\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(np.isnan(tx_mean_filled))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "tx_mean_filled[inds]=np.take(mean,inds[1])\n",
    "print(np.mean(tx_mean_filled, axis=0))\n",
    "print(np.std(tx_mean_filled, axis=0))\n",
    "print(np.min(tx_mean_filled, axis=0))\n",
    "print(np.max(tx_mean_filled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -9.73054681e-13   4.42750414e-15  -3.50538043e-15   7.10211001e-15\n",
      "  -7.23440168e-12  -6.30188342e-12   6.80633033e-13   2.44147274e-14\n",
      "   6.40282893e-15   2.86169444e-15  -6.95043934e-15   5.43928191e-15\n",
      "   5.55859610e-13  -5.97410332e-15   1.30739863e-16   6.37561115e-17\n",
      "   2.58283965e-14  -1.17813315e-16  -1.43046242e-16   8.24421384e-15\n",
      "   1.23416388e-16  -8.96501851e-15  -1.91224281e-15   2.89132939e-12\n",
      "  -2.77659665e-15   2.53467926e-14  -8.40723363e-12   2.09942845e-14\n",
      "  -5.89750587e-15  -3.38248540e-16]\n",
      "[ 0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998]\n",
      "[-2.1386682  -1.39312431 -1.8333427  -0.90951758 -2.56127609 -1.67194698\n",
      " -8.93117038 -2.76544782 -0.84932039 -0.97080623 -1.64619242 -1.07717127\n",
      " -2.13397163 -0.83470247 -2.04931283 -1.72495193 -0.93633718 -1.96484396\n",
      " -1.75356337 -1.26489201 -1.72819864 -1.55035529 -1.0017901  -1.16636921\n",
      " -3.25140511 -2.2274027  -1.60647971 -4.10084645 -3.20862439 -0.74543792]\n",
      "[  20.28757438   18.13091681   31.06073209   43.62694684    6.49901149\n",
      "   21.48715675    9.06964988    4.22896045  126.43196882   14.64079734\n",
      "   21.70528997    1.29216178    2.52240873   32.37988439    2.06574183\n",
      "    1.73394713   23.27725352    1.99410519    1.7056247    85.14746586\n",
      "    1.73936635   14.18328722    2.067495     22.03614698    3.25614162\n",
      "    2.24433141   38.52470255    4.12249259    3.2118577    15.91958241]\n"
     ]
    }
   ],
   "source": [
    "# NUMPY ONLY VERSION\n",
    "\n",
    "#Lets normalize\n",
    "tx_mean_filled_normalized = tx_mean_filled.copy()\n",
    "tx_mean_filled_normalized = (tx_mean_filled_normalized - tx_mean_filled_normalized.mean(axis=0)) / tx_mean_filled_normalized.std(axis=0, ddof=1)\n",
    "print(np.mean(tx_mean_filled_normalized, axis=0))\n",
    "print(np.std(tx_mean_filled_normalized, axis=0))\n",
    "print(np.min(tx_mean_filled_normalized, axis=0))\n",
    "print(np.max(tx_mean_filled_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.03280465e+02   4.92398193e+01   8.11819816e+01   5.78959617e+01\n",
      "   6.97496600e-01   1.07881121e+02  -2.38430900e-01   2.37309984e+00\n",
      "   1.89173324e+01   1.58432217e+02   1.43760943e+00  -1.28304708e-01\n",
      "   1.32982868e-01   3.87074191e+01  -1.09730480e-02  -8.17107200e-03\n",
      "   4.66602072e+01  -1.95074680e-02   4.35429640e-02   4.17172345e+01\n",
      "  -1.01191920e-02   2.09797178e+02   9.79176000e-01   5.09227808e+01\n",
      "  -1.96589200e-03  -7.44000800e-03   1.67369685e+01  -3.43716400e-03\n",
      "  -4.59136000e-04   7.30645914e+01]\n",
      "[  68.56596553   35.34481492   40.82860887   63.65555431    1.43904749\n",
      "  272.69840421    1.96648005    0.78290955   22.2734492   115.70588372\n",
      "    0.84474126    1.19358245    0.29896744   22.41203584    1.21407622\n",
      "    1.81675941   22.06487828    1.26497962    1.81660763   32.8946274\n",
      "    1.81221908  126.49925272    0.97742435   62.73325086    1.38270049\n",
      "    1.40505806   31.33881741    1.09445695    0.97874101   98.01546598]\n",
      "[  0.      0.      6.329   0.      0.      0.    -18.066   0.208   0.\n",
      "  46.104   0.047  -1.414   0.     20.     -2.499  -3.142  26.     -2.505\n",
      "  -3.142   0.109  -3.142  13.678   0.      0.     -4.499  -3.142   0.     -4.5\n",
      "  -3.142   0.   ]\n",
      "[  1.19202600e+03   6.90075000e+02   1.34935100e+03   2.83499900e+03\n",
      "   8.50300000e+00   4.97497900e+03   1.66900000e+01   5.68400000e+00\n",
      "   2.83499900e+03   1.85246200e+03   1.97730000e+01   1.41400000e+00\n",
      "   1.00000000e+00   7.64408000e+02   2.49700000e+00   3.14200000e+00\n",
      "   5.60271000e+02   2.50300000e+00   3.14200000e+00   2.84261700e+03\n",
      "   3.14200000e+00   2.00397600e+03   3.00000000e+00   1.12057300e+03\n",
      "   4.49900000e+00   3.14100000e+00   7.21456000e+02   4.50000000e+00\n",
      "   3.14200000e+00   1.63343300e+03]\n"
     ]
    }
   ],
   "source": [
    "# PURE NUMPY VERSION\n",
    "\n",
    "#Lets fill NaNs with 0\n",
    "tx_zero_filled = tx_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_zero_filled)\n",
    "tx_zero_filled[where_are_NaNs] = 0\n",
    "print(np.mean(tx_zero_filled, axis=0))\n",
    "print(np.std(tx_zero_filled, axis=0))\n",
    "print(np.min(tx_zero_filled, axis=0))\n",
    "print(np.max(tx_zero_filled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.23364047e-14   4.42750414e-15  -3.50538043e-15   7.10211001e-15\n",
      "  -2.93632718e-15   4.56355265e-15  -1.32019273e-15   2.44147274e-14\n",
      "   6.40282893e-15   2.86169444e-15  -6.95043934e-15   5.43928191e-15\n",
      "  -7.47514495e-16  -5.97410332e-15   1.30739863e-16   6.37561115e-17\n",
      "   2.58283965e-14  -1.17813315e-16  -1.43046242e-16   8.24421384e-15\n",
      "   1.23416388e-16  -8.96501851e-15  -1.91224281e-15  -3.99443056e-15\n",
      "  -1.46157516e-15   1.15313340e-15   8.03767719e-16   2.29318138e-15\n",
      "   9.35779405e-18  -3.38248540e-16]\n",
      "[ 0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998  0.999998\n",
      "  0.999998  0.999998]\n",
      "[-1.50629043 -1.39312431 -1.8333427  -0.90951758 -0.48469228 -0.3956052\n",
      " -9.06570778 -2.76544782 -0.84932039 -0.97080623 -1.64619242 -1.07717127\n",
      " -0.4448063  -0.83470247 -2.04931283 -1.72495193 -0.93633718 -1.96484396\n",
      " -1.75356337 -1.26489201 -1.72819864 -1.55035529 -1.0017901  -0.81173346\n",
      " -3.2523494  -2.2309069  -0.53406403 -4.1084794  -3.20977107 -0.74543792]\n",
      "[  15.87877235   18.13091681   31.06073209   43.62694684    5.42406547\n",
      "   17.84787908    8.60847639    4.22896045  126.43196882   14.64079734\n",
      "   21.70528997    1.29216178    2.90003285   32.37988439    2.06574183\n",
      "    1.73394713   23.27725352    1.99410519    1.7056247    85.14746586\n",
      "    1.73936635   14.18328722    2.067495     17.0507357     3.25519295\n",
      "    2.24078548   22.48705217    4.11476043    3.21070929   15.91958241]\n"
     ]
    }
   ],
   "source": [
    "# NUMPY ONLY VERSION\n",
    "\n",
    "#Lets normalize\n",
    "tx_zero_filled_normalized = tx_zero_filled.copy()\n",
    "tx_zero_filled_normalized = (tx_zero_filled_normalized - tx_zero_filled_normalized.mean(axis=0)) / tx_zero_filled_normalized.std(axis=0, ddof=1)\n",
    "print(np.mean(tx_zero_filled_normalized, axis=0))\n",
    "print(np.std(tx_zero_filled_normalized, axis=0))\n",
    "print(np.min(tx_zero_filled_normalized, axis=0))\n",
    "print(np.max(tx_zero_filled_normalized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PURE NUMPY VERSION\n",
    "# Lets do exactly the same for Predtiction dataset\n",
    "DATA_PRED_PATH = '../data/test.csv'\n",
    "_, tx_pred, ids_pred = load_csv_data(DATA_PRED_PATH)\n",
    "\n",
    "# Lets replace -999 values for nan's\n",
    "tx_pred_nan = tx_pred.copy()\n",
    "tx_pred_nan[tx_pred_nan==-999]=np.nan\n",
    "\n",
    "#Lets fill NaNs with column's mean value\n",
    "tx_pred_mean_filled = tx_pred_nan.copy()\n",
    "#Obtain mean of columns as you need, nanmean is just convenient.\n",
    "mean = np.nanmean(tx_pred_mean_filled, axis=0)\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(np.isnan(tx_pred_mean_filled))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "tx_pred_mean_filled[inds]=np.take(mean,inds[1])\n",
    "\n",
    "#Lets normalize tx_pred_mean_filled\n",
    "tx_pred_mean_filled_normalized = tx_pred_mean_filled.copy()\n",
    "tx_pred_mean_filled_normalized = (tx_pred_mean_filled_normalized - tx_pred_mean_filled_normalized.mean(axis=0)) / tx_pred_mean_filled_normalized.std(axis=0, ddof=1)\n",
    "\n",
    "#Lets fill NaNs with 0\n",
    "tx_pred_zero_filled = tx_pred_nan.copy()\n",
    "where_are_NaNs = np.isnan(tx_pred_zero_filled)\n",
    "tx_pred_zero_filled[where_are_NaNs] = 0\n",
    "\n",
    "#Lets normalize tx_pred_zero_filled\n",
    "tx_pred_zero_filled_normalized = tx_pred_zero_filled.copy()\n",
    "tx_pred_zero_filled_normalized = (tx_pred_zero_filled_normalized - tx_pred_zero_filled_normalized.mean(axis=0)) / tx_pred_zero_filled_normalized.std(axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save/load for future to/from csv\n",
    "# np.savetxt(\"../data/tx_zero_filled_normalized.csv\", tx_zero_filled_normalized, delimiter=\",\")\n",
    "# np.savetxt(\"../data/tx_pred_zero_filled_normalized.csv\", tx_pred_zero_filled_normalized, delimiter=\",\")\n",
    "# np.savetxt(\"../data/tx_mean_filled_normalized.csv\", tx_mean_filled_normalized, delimiter=\",\")\n",
    "# np.savetxt(\"../data/tx_pred_mean_filled_normalized.csv\", tx_pred_mean_filled_normalized, delimiter=\",\")\n",
    "\n",
    "\n",
    "# tx_zero_filled_normalized = np.loadtxt(\"../data/tx_zero_filled_normalized.csv\", delimiter=\",\")\n",
    "# tx_pred_zero_filled_normalized = np.loadtxt(\"../data/tx_pred_zero_filled_normalized.csv\", delimiter=\",\")\n",
    "# tx_mean_filled_normalized = np.loadtxt(\"../data/tx_mean_filled_normalized.csv\", delimiter=\",\")\n",
    "# tx_pred_mean_filled_normalized = np.loadtxt(\"../data/tx_pred_mean_filled_normalized.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets split tx to train and test\n",
    "split_ratio = 0.2\n",
    "tx_zero_filled_normalized_train, tx_zero_filled_normalized_test, y_zero_filled_normalized_train, y_zero_filled_normalized_test = split_data(tx_zero_filled_normalized, y, split_ratio)\n",
    "tx_mean_filled_normalized_train, tx_mean_filled_normalized_test, y_mean_filled_normalized_train, y_mean_filled_normalized_test = split_data(tx_mean_filled_normalized, y, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datasets = [tx_train, tx_zero_filled_normalized_train, tx_mean_filled_normalized_train]\n",
    "test_datasets = [tx_test, tx_zero_filled_normalized_test, tx_mean_filled_normalized_test]\n",
    "pred_datasets = [tx_pred, tx_pred_zero_filled_normalized, tx_pred_mean_filled_normalized]\n",
    "datasets_names = ['Original/Raw','Zero filled', 'Mean filled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 100\n",
    "gammas = np.logspace(-14, -18, 10)\n",
    "for i in range(len(train_datasets)):\n",
    "     logistic_regression_dataset_gammas_test(y_train, y_test, train_datasets[i], test_datasets[i], max_iters, gammas, datasets_names[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Logistic regression\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-14, -18, 10)# np.logspace(-16, -20, 10)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "weights = np.empty((0,new_meanfilled_tx_train.shape[1]), float)\n",
    "for gamma in np.nditer(gammas):\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    initial_w = np.zeros((new_meanfilled_tx_train.shape[1],1))\n",
    "    logistic_regression_w, logistic_regression_loss = logistic_regression(np.array([y_train]).T, new_meanfilled_tx_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    train_losses = np.append(train_losses, logistic_regression_loss)\n",
    "    test_mse = compute_loss(y_test, new_meanfilled_tx_test, logistic_regression_w[:,0])\n",
    "    test_rmse = np.sqrt(2*test_mse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    weights = np.vstack((weights, logistic_regression_w.T))\n",
    "    print(\"Logistic Regression: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=logistic_regression_loss))\n",
    "\n",
    "\n",
    "plt.semilogx(gammas, test_losses, marker=\".\", color='r', label='test error')\n",
    "plt.semilogx(gammas, train_losses, marker=\".\", color='b', label='train error')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Least Squares \n",
    "start_time = datetime.datetime.now()\n",
    "least_squares_w, least_squares_loss = least_squares(y_train, new_tx_train)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "test_mse = compute_loss(y_test, new_tx_test, least_squares_w)\n",
    "test_rmse = np.sqrt(2*test_mse)\n",
    "\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Lest Squares: execution time={t:.3f} seconds. RMSE Train Loss={l}, Test Loss={tl}\".format(t=exection_time, l=least_squares_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "subset_y = y\n",
    "subset_tx = tx\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "seed = 1\n",
    "k_fold = 10\n",
    "lambdas = np.logspace(-16, 2, 1)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "# weights = np.empty((0,subset_tx.shape[1]), float)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for lambd in np.nditer(lambdas):\n",
    "    loss_tr, loss_te = cross_validation_mat(subset_y, subset_tx, k_fold, seed, lambd)\n",
    "    rmse_tr = np.append(rmse_tr, loss_tr)\n",
    "    rmse_te = np.append(rmse_te, loss_te)\n",
    "#     weights = np.vstack((weights, w))\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"Cross Validation: execution time={t:.3f} seconds.\".format(t=exection_time))\n",
    "#cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/logistic_regression_cross_validation_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "weights_pred = least_squares_w\n",
    "y_pred = predict_labels(weights_pred, new_tx_pred)\n",
    "create_csv_submission(ids_pred, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://inclass.kaggle.com/c/epfml-project-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
