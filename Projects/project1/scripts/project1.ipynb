{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets verify loaded data\n",
    "print(y.shape)\n",
    "print(tx.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "\n",
    "# ax2 = fig.add_subplot(1, 1, 1)\n",
    "# ax2.scatter(tX[:,0].T, y, marker=\".\", color='b', s=5)\n",
    "# ax2.set_xlabel(\"x\")\n",
    "# ax2.set_ylabel(\"y\")\n",
    "# ax2.grid()\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Criteria:\n",
    "1. Competitive Part **(counts one third)**. The final rank of your team in the (private) leaderboard will be translated linearly to a scale from 4 to 6.\n",
    "2. Code **(counts one third)**. In Python. No external libraries allowed! For this first project, we want you to implement and use the methods we have seen in class. The code will be graded by two TAs independently, according to the criteria described:\n",
    "* Rules for the code part:\n",
    "  * Reproducibility: In your submission, you must provide a script run.py which produces exactly the same .csv predictions which you used in your best submission to the competition on Kaggle.\n",
    "  * Documentation: Your ML system must be clearly described in your PDF report and also well- documented in the code itself. A clear ReadMe file must be provided. The documentation must also include all data preparation, feature generation as well as cross-validation steps that you have used.\n",
    "  * In addition to your customized system, don’t forget that your code submission must still also include the 6 basic method implementations as described above in step 2.\n",
    "  * No use of external ML libraries is allowed in Project 1. (It will be allowed in Project 2).\n",
    "  * No external datasets allowed.\n",
    "3. Written Report **(counts one third)**. You will write a maximum 2 page PDF report on your findings, using LaTeX. The code will be graded by two TAs independently, and we will provide you feedback. The main criteria will be if you were able to correctly use, implement and describe the 6 baseline methods mentioned in Step 2 above. This counts half for the written report. In addition, we will grade you on the scientific contribution you made additionally, to improve your predictions. For this part, the criteria are\n",
    "  * scientific novelty\n",
    "  * creativity\n",
    "  * reproducibility\n",
    "  * solid comparison baselines supporting your claims – writeup quality\n",
    "  \n",
    "\n",
    "As usual, your code and report will be automatically checked for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo's\n",
    "\n",
    "* verify correctness of implemented methods\n",
    "* (!) implement local estimation on local validation test set and local **cross validation**!\n",
    "* fix and check reg_logistic_regression\n",
    "* Exploratory data analysis with comments\n",
    "* Dataset cleaning\n",
    "* Comment code and this notebook\n",
    "* Improve predictions to be number one in the keggle!\n",
    "  * construct better features (optional)\n",
    "  * implement additional modifications of basic methods implemented (optional)\n",
    "  * clean and preprocess data\n",
    "* LateX pdf report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.0000001\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "grad_loss, gradient_w = least_squares_GD(y, tx, gamma, max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds. MSE Loss={l}\".format(t=exection_time, l=grad_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = np.logspace(-1, -10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.0001\n",
    "gammas = np.logspace(-1, -10, 10)\n",
    "for gamma in np.nditer(gammas):\n",
    "    # Start stochastic gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    stoch_grad_loss, stoch_gradient_w = least_squares_SGD(y, tx, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Stochastic Gradient Descent: execution time={t:.3f} seconds. MSE Loss={l}\".format(t=exection_time, l=stoch_grad_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least Squares - produce our best keggle result 57th position Mateusz Paluchowski0.74463\n",
    "start_time = datetime.datetime.now()\n",
    "least_squares_loss, leas_squares_gradient_w = least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Stochastic Gradient Descent: execution time={t:.3f} seconds. MSE Loss={l}\".format(t=exection_time, l=least_squares_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambs = np.logspace(-4, -20, 100)\n",
    "lambs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ridge Regression - to be checked because changes in lamb parameter almost doesnt affect anything (only large lambs)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "lamb = 0.0001\n",
    "\n",
    "lambs = np.logspace(-4, -20, 100)\n",
    "for lamb in np.nditer(lambs):\n",
    "    start_time = datetime.datetime.now()\n",
    "    ridge_regression_loss, ridge_regression_gradient_w = ridge_regression(y, tx, lamb)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Ridge Regression: execution time={t:.3f} seconds. MSE Loss={l}\".format(t=exection_time, l=least_squares_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.zeros((tx.shape[1],1))\n",
    "\n",
    "print(tx.shape)\n",
    "print(np.array([y]).T.shape)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gammas = np.logspace(-16, -20, 10)\n",
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20. , -21.5, -23. ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas = np.logspace(-20, -23, 3)\n",
    "gammas\n",
    "\n",
    "# For 1000 iters\n",
    "# 1e-20: MSE Loss=-65117.90563844488\n",
    "# 1e-21: MSE Loss=149446.2867804076\n",
    "# 5e-22: MSE Loss=161366.53989681418\n",
    "# 1e-23: MSE Loss=173048.39001428057\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: execution time=868.312 seconds. MSE Loss=-1018728.2017495089\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression using gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 5e-20 #best from linspace below\n",
    "\n",
    "gammas = np.logspace(-21, -23, 10)# np.logspace(-16, -20, 10)\n",
    "for gamma in np.nditer(np.array([gamma])):\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    logistic_regression_loss, logistic_regression_w = logistic_regression(np.array([y]).T, tx, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic Regression: execution time={t:.3f} seconds. MSE Loss={l}\".format(t=exection_time, l=logistic_regression_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Regularized Logistic Regression using gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1\n",
    "gamma = 3.41379310345e-14\n",
    "lambd = 0.1\n",
    "    \n",
    "start_time = datetime.datetime.now()\n",
    "logistic_regression_loss, logistic_regression_w = reg_logistic_regression(np.array([y]).T, tx, lambd, gamma, max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Penalizec Logistic Regression: execution time={t:.3f} seconds. MSE Loss={l}\".format(t=exection_time, l=logistic_regression_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/logistic_regression_100iter_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "weights = logistic_regression_w\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://inclass.kaggle.com/c/epfml-project-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
