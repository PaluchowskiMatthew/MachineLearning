{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from functions import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** LS ****\n",
      "0.843705415289\n",
      "0.846162529975\n",
      "\t*** GS ****\n",
      "3.66057618293\n",
      "3.69187552172\n",
      "\t*** N ****\n",
      "4.40101314193\n",
      "4.43857907606\n",
      "\t*** CLS ****\n",
      "-mean weights:\n",
      "0.945358319202\n",
      "0.944921623947\n",
      "-best weights:\n",
      "0.945353938203\n",
      "0.944920581668\n",
      "\t**** Penalized *******\n",
      "4.40103100733\n",
      "4.43859711086\n"
     ]
    }
   ],
   "source": [
    "#Let's modify the data: replace -999 by -99\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -99)\n",
    "#PCA\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x_99.T, full_matrices=False)\n",
    "x_proj = np.dot(x_99, eigenvectors[:, 0:15])\n",
    "\n",
    "ratio = 0.1\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "\n",
    "tx_train_99, tr_mean_99, tr_std_99 = standardize(x_train)\n",
    "tx_test_99, te_mean_99, te_std_99 = standardize(x_test)\n",
    "\n",
    "#Least squares\n",
    "print(\"\\t*** LS ****\")\n",
    "w_LS, loss = least_squares(y_train, tx_train_99)\n",
    "test_mse = compute_loss(y_test, tx_test_99, w_LS)\n",
    "train_mse = compute_loss(y_train, tx_train_99, w_LS)\n",
    "rmse_te = np.sqrt(2*test_mse)\n",
    "rmse_tr = np.sqrt(2*train_mse)\n",
    "print(rmse_te)\n",
    "print(rmse_tr)\n",
    "\n",
    "#Gradient Descent\n",
    "print(\"\\t*** GS ****\")\n",
    "alpha = 0.00001\n",
    "max_iters = 5\n",
    "loss_GD, w_GD = learning_by_gradient_descent(y_train, tx_train_99, alpha, max_iters)\n",
    "test_mse_GD = compute_loss(y_test, tx_test_99, w_GD)\n",
    "train_mse_GD = compute_loss(y_train, tx_train_99, w_GD)\n",
    "rmse_te_GD = np.sqrt(2*test_mse_GD)\n",
    "rmse_tr_GD = np.sqrt(2*train_mse_GD)\n",
    "print(rmse_te_GD)\n",
    "print(rmse_tr_GD)\n",
    "\n",
    "#Newton\n",
    "print(\"\\t*** N ****\")\n",
    "alpha = 0.00002\n",
    "max_iters = 5\n",
    "lambd = 0.5\n",
    "loss_N, w_N = learning_by_newton_method(y_train, tx_train_99, alpha, max_iters)\n",
    "test_mse_N = compute_loss(y_test, tx_test_99, w_N)\n",
    "train_mse_N = compute_loss(y_train, tx_train_99, w_N)\n",
    "rmse_te_N = np.sqrt(2*test_mse_N)\n",
    "rmse_tr_N = np.sqrt(2*train_mse_N)\n",
    "print(rmse_te_N)\n",
    "print(rmse_tr_N)\n",
    "\n",
    "#Mean of 10 folds least squares\n",
    "print(\"\\t*** CLS ****\")\n",
    "k_folds = 10\n",
    "seed = 1\n",
    "w_LS_folds, rmse_tr_CLS, rmse_te_CLS = cross_validation_LS(y, x_proj, k_folds, seed)\n",
    "\n",
    "#Mean of weights along folds\n",
    "w_CLS = w_LS_folds.mean(axis=0)\n",
    "w_CLS = np.reshape(w_CLS, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_mean = compute_loss(y_test, tx_test_99, w_CLS)\n",
    "train_mse_CLS_mean = compute_loss(y_train, tx_train_99, w_CLS)\n",
    "rmse_te_CLS_mean = np.sqrt(2*test_mse_CLS_mean)\n",
    "rmse_tr_CLS_mean = np.sqrt(2*train_mse_CLS_mean)\n",
    "print(\"-mean weights:\")\n",
    "print(rmse_te_CLS_mean)\n",
    "print(rmse_tr_CLS_mean)\n",
    "\n",
    "#Best weights in test results\n",
    "w_CLS_best = w_LS_folds[np.argmin(rmse_te_CLS)]\n",
    "w_CLS_best = np.reshape(w_CLS_best, (tx_train_99.shape[1], 1))\n",
    "test_mse_CLS_best = compute_loss(y_test, tx_test_99, w_CLS_best)\n",
    "train_mse_CLS_best = compute_loss(y_train, tx_train_99, w_CLS_best)\n",
    "rmse_te_CLS_best = np.sqrt(2*test_mse_CLS_best)\n",
    "rmse_tr_CLS_best = np.sqrt(2*train_mse_CLS_best)\n",
    "print(\"-best weights:\")\n",
    "print(rmse_te_CLS_best)\n",
    "print(rmse_tr_CLS_best)\n",
    "\n",
    "#Penalized Regression\n",
    "lambd = 0.05\n",
    "gamma = 0.00002\n",
    "max_iters = 5\n",
    "print(\"\\t**** Penalized *******\")\n",
    "loss_reg, w_reg = reg_logistic_regression(y_train, tx_train_99, lambd, gamma, max_iters)\n",
    "test_mse_reg = compute_loss(y_test, tx_test_99, w_reg)\n",
    "train_mse_reg = compute_loss(y_train, tx_train_99, w_reg)\n",
    "rmse_te_reg = np.sqrt(2*test_mse_reg)\n",
    "rmse_tr_reg = np.sqrt(2*train_mse_reg)\n",
    "print(rmse_te_reg)\n",
    "print(rmse_tr_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IRLS\n",
    "x_99 = x\n",
    "np.putmask(x_99, x_99==-999, -99)\n",
    "#PCA to avoid correlation and singular matrix\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x_99.T, full_matrices=False)\n",
    "x_proj = np.dot(x_99, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "ratio = 0.05\n",
    "x_train_PCA, x_test_PCA, y_train_PCA, y_test_PCA = split_data(x_proj, y, ratio)\n",
    "tx_train_PCA = np.c_[np.ones((y_train_PCA.shape[0], 1)), x_train_PCA]\n",
    "tx_test_PCA = np.c_[np.ones((y_test_PCA.shape[0], 1)), x_test_PCA]\n",
    "\n",
    "max_iters = 5\n",
    "print(\"\\t*** IRLS *****\")\n",
    "loss_IRLS, w_IRLS = learning_by_IRLS(y_train_PCA, tx_train_PCA, max_iters)\n",
    "test_mse_IRLS = compute_loss(y_test_PCA, tx_test_PCA, w_IRLS)\n",
    "train_mse_IRLS = compute_loss(y_train_PCA, tx_train_PCA, w_IRLS)\n",
    "rmse_te_IRLS = np.sqrt(2*test_mse_IRLS)\n",
    "rmse_tr_IRLS = np.sqrt(2*train_mse_IRLS)\n",
    "print(rmse_te_IRLS)\n",
    "print(rmse_tr_IRLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,k_folds), rmse_tr_CLS)\n",
    "plt.plot(range(0,k_folds), rmse_te_CLS, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_w = w_LS.mean(axis=0)\n",
    "std_w = w_LS.std(axis=0)\n",
    "rmse_tr = compute_loss(y_train, tx_train_99, mean_w)\n",
    "rmse_te = compute_loss(y_test, tx_test_99, mean_w)\n",
    "print(rmse_tr)\n",
    "print(rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_s = standardize(x)\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:12])\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)\n",
    "print(V.shape)\n",
    "print(x_proj.shape)\n",
    "sigma = x_proj.std(axis=0).mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "\n",
    "loss, w = learning_by_newton_method(y_train, tx_train, alpha, 100)\n",
    "rmse  = compute_loss(y_test, tx_test, w)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "#alpha = 0.001\n",
    "lambd = 0.5\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-5, -1, 10)\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "for gamma in gammas:\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, gamma, max_iter)\n",
    "    lossREG = compute_loss(y_test, tx_test, w)\n",
    "    rmse = np.sqrt(2*lossREG)\n",
    "    losses.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(gammas[0:5], losses[0:5], marker=\".\", color='b')\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights_std = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try1, x_mean, std_x = standardize(x)\n",
    "#try1 = np.reshape(try1, (x.shape[0], x.shape[1]))\n",
    "print(try1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = w\n",
    "print(\"RMSE: {l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "k_fold = 10\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation(y_vec, x, k_fold, seed, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "#w = np.ones((x_train.shape[1]+1, 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_RegPen, tr_rmse, te_rmse = cross_validation(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenvectors, eigenvalues, V = np.linalg.svd(X_test.T, full_matrices=False)\n",
    "x_proj_test = np.dot(X_test, eigenvectors[:, 0:15])\n",
    "\n",
    "tX_test_proj = np.c_[np.ones((x_proj_test.shape[0], 1)), x_proj_test]\n",
    "\n",
    "weights = w_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test_proj)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
