{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from functions import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets verify loaded data\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(ids.shape)\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "#y_vec = np.zeros((y.shape[0], 1))\n",
    "#y_vec[:,0] = y\n",
    "\n",
    "print(y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_s = standardize(x)\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:12])\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)\n",
    "print(V.shape)\n",
    "print(x_proj.shape)\n",
    "sigma = x_proj.std(axis=0).mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape[0]+x_train.shape[0])\n",
    "print(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "losses.append(loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laz/Documents/PCML/MachineLearning/Projects/project1/scripts/functions.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  gradient_penalty = lambd * 2 * w / np.linalg.norm(w)\n"
     ]
    }
   ],
   "source": [
    "max_iter = 5\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "#lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "lambds = np.logspace(-9, -1, 10)\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "for lamb in lambds:\n",
    "    loss, w = reg_logistic_regression(y_train, tx_train, lamb, alpha, max_iter)\n",
    "    lossREG = compute_loss(y_test, tx_test, w)\n",
    "    rmse = np.sqrt(2*lossREG)\n",
    "    losses.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data has no positive values, and therefore can not be log-scaled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b40d6071029d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemilogx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lambda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rmse\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/laz/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msemilogx\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3266\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3267\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3268\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemilogx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3269\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3270\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/laz/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36msemilogx\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1614\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# we've already processed the hold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1616\u001b[1;33m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m  \u001b[1;31m# restore the hold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/laz/anaconda3/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1810\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1811\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1812\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/laz/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1426\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1428\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1429\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/laz/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mautoscale_view\u001b[1;34m(self, tight, scalex, scaley)\u001b[0m\n\u001b[0;32m   2171\u001b[0m                 \u001b[0mx1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_tight\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2173\u001b[1;33m                 \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlocator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/laz/anaconda3/lib/python3.5/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mview_limits\u001b[1;34m(self, vmin, vmax)\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mminpos\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1613\u001b[1;33m                 \u001b[1;34m\"Data has no positive values, and therefore can not be \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1614\u001b[0m                 \"log-scaled.\")\n\u001b[0;32m   1615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data has no positive values, and therefore can not be log-scaled."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEECAYAAADJSpQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd225927f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(lambds, losses, marker=\".\", color='b')\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights_std = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try1, x_mean, std_x = standardize(x)\n",
    "#try1 = np.reshape(try1, (x.shape[0], x.shape[1]))\n",
    "print(try1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = w\n",
    "print(\"RMSE: {l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "k_fold = 10\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation(y_vec, x, k_fold, seed, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 40\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_LS, tr_rmse, te_rmse = cross_validation(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = best_weights_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
