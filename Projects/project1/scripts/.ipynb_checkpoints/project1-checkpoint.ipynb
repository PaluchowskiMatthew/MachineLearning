{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "from functions import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets verify loaded data\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(ids.shape)\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "#y_vec = np.zeros((y.shape[0], 1))\n",
    "#y_vec[:,0] = y\n",
    "\n",
    "print(y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_s = standardize(x)\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:12])\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)\n",
    "print(V.shape)\n",
    "print(x_proj.shape)\n",
    "sigma = x_proj.std(axis=0).mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "ratio = 0.1\n",
    "lambd = 0.01\n",
    "losses = []\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "w = np.zeros((tx_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape[0]+x_train.shape[0])\n",
    "print(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "losses.append(loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "#tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "\n",
    "tx_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "\n",
    "loss, w = reg_logistic_regression(y_train, tx_train, lambd, alpha, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32578127373e+40\n"
     ]
    }
   ],
   "source": [
    "lossREG = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*lossREG)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 1000\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "\n",
    "tx_train, x_tr_mean, x_tr_std = standardize(x_train)\n",
    "tx_test, x_te_mean, x_te_std = standardize(x_test)\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_loss(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights_std = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try1, x_mean, std_x = standardize(x)\n",
    "#try1 = np.reshape(try1, (x.shape[0], x.shape[1]))\n",
    "print(try1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = w\n",
    "print(\"RMSE: {l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "k_fold = 10\n",
    "seed = 1\n",
    "lambd = 0.01\n",
    "w_cv, lossTR_cv, lossTE_cv = cross_validation(y_vec, x, k_fold, seed, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "alpha = 0.002\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "# Initialization\n",
    "#w_initial = weights\n",
    "\n",
    "y_vec = np.zeros((y.shape[0], 1))\n",
    "y_vec[:,0] = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x, y_vec, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "print(\"The loss={l}\".format(l=compute_RMSE(y_test, tx_test, w)))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_iters = 1\n",
    "#gamma = 0.4\n",
    "#batch_size = 300\n",
    "max_iter = 10\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "\n",
    "eigenvectors, eigenvalues, V = np.linalg.svd(x.T, full_matrices=False)\n",
    "x_proj = np.dot(x, eigenvectors[:, 0:10])\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_proj, y, ratio)\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_newton_method(y_train, tx_train, w, alpha)\n",
    "    if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "#visualization(y_train, x_train, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "\n",
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(\"The loss={l}\".format(l=rmse))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Newton: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = compute_loss(y_test, tx_test, w)\n",
    "rmse = np.sqrt(2*mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 40\n",
    "threshold = 1e-8\n",
    "alpha = 0.001\n",
    "lambd = 0.001\n",
    "ratio = 0.1\n",
    "losses = []\n",
    "k_fold = 20\n",
    "seed = 1\n",
    "\n",
    "#x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "#tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "#tx_test = np.c_[np.ones((y_test.shape[0], 1)), x_test]\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "w_LS, tr_rmse, te_rmse = cross_validation(y, x, k_fold, seed, lambd, max_iter)\n",
    "print(w_LS.std(axis=0).mean())\n",
    "print(tr_rmse)\n",
    "print(te_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_NM_20folds = w_LS\n",
    "train_rmse_NM_20folds = tr_rmse\n",
    "test_rmse_NM_20folds = te_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = best_weights_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
