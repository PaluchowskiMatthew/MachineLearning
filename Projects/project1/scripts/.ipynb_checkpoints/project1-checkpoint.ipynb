{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "from matplotlib.mlab import PCA\n",
    "from functions import *\n",
    "from costs import *\n",
    "from method_comparison_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Github does not accept files above 100mb and test.csv is 104mb\n",
    "# thus we upload zip whith test.csv which needs to be extracted\n",
    "with zipfile.ZipFile(\"../data/test.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#Lets verify loaded data\n",
    "print(y.shape)\n",
    "print(tx.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "y_test, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "print(y_test.shape)\n",
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_y = np.append(y, y_test)\n",
    "all_tx = np.concatenate((tx, tx_test))\n",
    "\n",
    "print(all_y.shape)\n",
    "print(all_tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "\n",
    "# ax2 = fig.add_subplot(1, 1, 1)\n",
    "# ax2.scatter(tX[:,0].T, y, marker=\".\", color='b', s=5)\n",
    "# ax2.set_xlabel(\"x\")\n",
    "# ax2.set_ylabel(\"y\")\n",
    "# ax2.grid()\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Criteria:\n",
    "1. Competitive Part **(counts one third)**. The final rank of your team in the (private) leaderboard will be translated linearly to a scale from 4 to 6.\n",
    "2. Code **(counts one third)**. In Python. No external libraries allowed! For this first project, we want you to implement and use the methods we have seen in class. The code will be graded by two TAs independently, according to the criteria described:\n",
    "* Rules for the code part:\n",
    "  * Reproducibility: In your submission, you must provide a script run.py which produces exactly the same .csv predictions which you used in your best submission to the competition on Kaggle.\n",
    "  * Documentation: Your ML system must be clearly described in your PDF report and also well- documented in the code itself. A clear ReadMe file must be provided. The documentation must also include all data preparation, feature generation as well as cross-validation steps that you have used.\n",
    "  * In addition to your customized system, don’t forget that your code submission must still also include the 6 basic method implementations as described above in step 2.\n",
    "  * No use of external ML libraries is allowed in Project 1. (It will be allowed in Project 2).\n",
    "  * No external datasets allowed.\n",
    "3. Written Report **(counts one third)**. You will write a maximum 2 page PDF report on your findings, using LaTeX. The code will be graded by two TAs independently, and we will provide you feedback. The main criteria will be if you were able to correctly use, implement and describe the 6 baseline methods mentioned in Step 2 above. This counts half for the written report. In addition, we will grade you on the scientific contribution you made additionally, to improve your predictions. For this part, the criteria are\n",
    "  * scientific novelty\n",
    "  * creativity\n",
    "  * reproducibility\n",
    "  * solid comparison baselines supporting your claims – writeup quality\n",
    "  \n",
    "\n",
    "As usual, your code and report will be automatically checked for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo's\n",
    "\n",
    "* verify correctness of implemented methods\n",
    "* (!) implement local estimation on local validation test set and local **cross validation**! **CV DONE**\n",
    "* fix and check reg_logistic_regression\n",
    "* Exploratory data analysis with comments\n",
    "* Dataset cleaning\n",
    "* Comment code and this notebook\n",
    "* Improve predictions to be number one in the keggle!\n",
    "  * construct better features (optional)\n",
    "  * implement additional modifications of basic methods implemented (optional)\n",
    "  * clean and preprocess data\n",
    "* LateX pdf report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets test some basics: Least Squares Gradient Descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-7, -9, 10)\n",
    "losses = []\n",
    "for gamma in np.nditer(gammas):\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    grad_loss, gradient_w = least_squares_GD(y, tx, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    grad_loss = compute_rmse_loss(grad_loss)\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    losses = np.append(losses, grad_loss)\n",
    "    print(\"Gradient Descent: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=grad_loss))\n",
    "    \n",
    "plt.semilogx(gammas, losses, marker=\".\", color='b')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-3, -10, 10)\n",
    "losses = []\n",
    "for gamma in np.nditer(gammas):\n",
    "    # Start stochastic gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    stoch_grad_loss, stoch_gradient_w = least_squares_SGD(y, tx, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    stoch_grad_loss = compute_rmse_loss(stoch_grad_loss)\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    losses = np.append(losses, stoch_grad_loss)\n",
    "    print(\"Stochastic Gradient Descent: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=stoch_grad_loss))\n",
    "\n",
    "plt.semilogx(gammas, losses, marker=\".\", color='b')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least Squares - produce our best keggle result 57th position Mateusz Paluchowski0.74463\n",
    "start_time = datetime.datetime.now()\n",
    "least_squares_loss, least_squares_w = least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "least_squares_loss = compute_rmse_loss(least_squares_loss)\n",
    "\n",
    "test_mse = compute_loss(y_test, tx_test, least_squares_w)\n",
    "test_rmse = np.sqrt(2*test_mse)\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Lest Squares: execution time={t:.3f} seconds. RMSE Train Loss={l}, Test Loss={tl}\".format(t=exection_time, l=least_squares_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ridge Regression - to be checked because changes in lamb parameter almost doesnt affect anything (only large lambs)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lambs = np.array([0.00025])#np.logspace(1, 16, 100)\n",
    "start_time = datetime.datetime.now()\n",
    "for lamb in np.nditer(lambs):\n",
    "    ridge_regression_loss, ridge_regression_gradient_w = ridge_regression(y, new_zerofilled_tx, lamb)\n",
    "    \n",
    "    ridge_regression_loss = compute_rmse_loss(ridge_regression_loss)\n",
    "    train_losses = np.append(train_losses, ridge_regression_loss)\n",
    "    \n",
    "    test_mse = compute_loss(y_test, tx_test, ridge_regression_gradient_w)\n",
    "    test_rmse = np.sqrt(2*test_mse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(test_losses)\n",
    "print(train_losses)\n",
    "print(\"Ridge Regression: execution time={t:.3f} seconds.\".format(t=exection_time))\n",
    "plt.semilogx(lambs, train_losses, marker=\".\", color='b', label='Train')\n",
    "plt.semilogx(lambs, test_losses, marker=\".\", color='r', label='Test')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gammas = np.logspace(-20, -23, 3)\n",
    "gammas\n",
    "\n",
    "# For 1000 iters\n",
    "# 1e-20: MSE Loss=-65117.90563844488\n",
    "# 1e-21: MSE Loss=149446.2867804076\n",
    "# 5e-22: MSE Loss=161366.53989681418\n",
    "# 1e-23: MSE Loss=173048.39001428057\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression using gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "losses = []\n",
    "weights = np.empty((0,new_zerofilled_tx.shape[1]), float)\n",
    "gammas = np.logspace(-18, -23, 10)# np.logspace(-16, -20, 10)\n",
    "for gamma in np.nditer(gammas):\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    logistic_regression_loss, logistic_regression_w = logistic_regression(np.array([y]).T, tx, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    logistic_regression_loss = compute_rmse_loss(logistic_regression_loss)\n",
    "    losses = np.append(losses, logistic_regression_loss)\n",
    "    weights = np.vstack((weights, logistic_regression_w.T))\n",
    "    print(\"Logistic Regression: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=logistic_regression_loss))\n",
    "\n",
    "\n",
    "plt.semilogx(gammas, losses, marker=\".\", color='b', label='log reg rmse error')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Regularized Logistic Regression using gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1\n",
    "gamma = 3.41379310345e-14\n",
    "lambd = 0.1\n",
    "    \n",
    "start_time = datetime.datetime.now()\n",
    "logistic_regression_loss, logistic_regression_w = reg_logistic_regression(np.array([y]).T, tx, lambd, gamma, max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Penalized Logistic Regression: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=logistic_regression_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOOD: Logistic Regression using newtons method\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 5e-20 \n",
    "\n",
    "gammas = np.logspace(-18, -23, 10)\n",
    "for gamma in np.nditer(gammas[0]):\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    logistic_regression_newton_loss, logistic_regression_newton_w = logistic_regression_newton(np.array([y]).T, tx, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    logistic_regression_newton_loss = compute_rmse_loss(logistic_regression_newton_loss)\n",
    "    print(\"Logistic Regression: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=logistic_regression_newton_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets load it into Pandas data frame since it is easier for data analysis\n",
    "original_df = pd.DataFrame(tx)\n",
    "# original_df.columns = original_df.columns.astype(str)\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets display some basic statistics\n",
    "columns = original_df.columns.to_series()\n",
    "sparse_columns = np.array([])\n",
    "for i, column in columns.iteritems():\n",
    "    value_counts = original_df[original_df.columns[column]].value_counts()\n",
    "    if -999 in value_counts:\n",
    "        sparse_columns = np.append(sparse_columns, [column])\n",
    "        print(\"Column {c_no} contains {v_no} values equal to -999\".format(c_no = column, v_no = value_counts[-999]))\n",
    "\n",
    "print('Sparse columns:')        \n",
    "print(sparse_columns)\n",
    "first10_df = original_df.iloc[:,:10]\n",
    "first10_df.describe()\n",
    "# As we can see there are features (columns) which are afected greatly by missing values represented as -999\n",
    "# for example 4th, 5th, 6th. We should do something about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "middle10_df = original_df.iloc[:,10:20]\n",
    "middle10_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last10_df = original_df.iloc[:,20:]\n",
    "last10_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_df.isnull().sum()\n",
    "\n",
    "#At least there are no null values! (-999 are our nulls in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets replace -999 values for nan's\n",
    "\n",
    "replaced999_df = original_df.replace(-999, np.nan)\n",
    "replaced999_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_filled_df = replaced999_df.copy()\n",
    "for i, column in np.ndenumerate(sparse_columns):\n",
    "    col_mean = mean_filled_df[column].mean()\n",
    "    mean_filled_df[column] = mean_filled_df[column].fillna(col_mean)\n",
    "mean_filled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_filled_normalized_df = (mean_filled_df - mean_filled_df.mean())\n",
    "mean_filled_normalized_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replaced999_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_ = pd.scatter_matrix(replaced999_df.loc[:,0:10], figsize=(20,20), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_ = pd.scatter_matrix(replaced999_df.loc[:,10:20], figsize=(20,20), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_ = pd.scatter_matrix(replaced999_df.loc[:,20:30], figsize=(20,20), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets sum all sprase columns and combine it into new one\n",
    "combined_df = replaced999_df.copy()\n",
    "combined_df['combined'] = replaced999_df[sparse_columns].sum(axis=1)\n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value_counts = combined_df['combined'].value_counts()\n",
    "np.nan in value_counts\n",
    "\n",
    "#No NaN's! Impressive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_filled_df = replaced999_df.fillna(0)\n",
    "zero_filled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "droped_nans_df = combined_df.dropna(axis=1, thresh=250000)\n",
    "print(droped_nans_df.columns.shape)\n",
    "droped_nans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_ = pd.scatter_matrix(droped_nans_df, figsize=(20,20), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_filled_normalized_df = (zero_filled_df - zero_filled_df.mean())\n",
    "zero_filled_normalized_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_df = (droped_nans_df - droped_nans_df.mean())# / (droped_nans_df.max() - droped_nans_df.min())\n",
    "normalized_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets do exactly the same but for test dataset\n",
    "original_test_df = pd.DataFrame(tx_test)\n",
    "replaced999_test_df = original_test_df.replace(-999, np.nan)\n",
    "mean_filled_test_df = replaced999_test_df.copy()\n",
    "for i, column in np.ndenumerate(sparse_columns):\n",
    "    col_mean = mean_filled_test_df[column].mean()\n",
    "    mean_filled_test_df[column] = mean_filled_test_df[column].fillna(col_mean)\n",
    "mean_filled_normalized_test_df = (mean_filled_test_df - mean_filled_test_df.mean())\n",
    "combined_test_df = replaced999_test_df.copy()\n",
    "combined_test_df['combined'] = combined_test_df[sparse_columns].sum(axis=1)\n",
    "droped_nans_test_df = combined_test_df.dropna(axis=1, thresh=568000)\n",
    "normalized_test_df = (droped_nans_test_df - droped_nans_test_df.mean())# / (droped_nans_test_df.max() - droped_nans_test_df.min())\n",
    "\n",
    "zero_filled_test_df = replaced999_test_df.fillna(0)\n",
    "zero_filled_test_normalized_df = (zero_filled_test_df - zero_filled_test_df.mean())\n",
    "#Verify we left same columns\n",
    "print(droped_nans_df.columns == droped_nans_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets change it back to numpy array\n",
    "\n",
    "new_tx = normalized_df.as_matrix()\n",
    "new_meanfilled_tx = mean_filled_normalized_df.as_matrix()\n",
    "new_zerofilled_tx = zero_filled_normalized_df.as_matrix()[:,0:30]\n",
    "new_tx_test = normalized_test_df.as_matrix()\n",
    "new_meanfilled_tx_test = mean_filled_normalized_test_df.as_matrix()\n",
    "new_zerofilled_tx_test = zero_filled_test_normalized_df.as_matrix()[:,0:30]\n",
    "new_all_tx = np.concatenate((new_tx, new_tx_test)) \n",
    "new_all_zerofilled_tx = np.concatenate((new_zerofilled_tx, new_zerofilled_tx_test)) \n",
    "print(new_tx.shape)\n",
    "print(new_tx_test.shape)\n",
    "print(new_all_tx.shape)\n",
    "print(new_all_zerofilled_tx.shape)\n",
    "\n",
    "print(new_meanfilled_tx.shape)\n",
    "print(new_meanfilled_tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save/load for future to/from csv\n",
    "# np.savetxt(\"new_tx.csv\", new_tx, delimiter=\",\")\n",
    "# np.savetxt(\"new_tx_test.csv\", new_tx_test, delimiter=\",\")\n",
    "# np.savetxt(\"new_zerofilled_tx.csv\", new_zerofilled_tx, delimiter=\",\")\n",
    "# np.savetxt(\"new_zerofilled_tx_test.csv\", new_zerofilled_tx_test, delimiter=\",\")\n",
    "# np.savetxt(\"new_meanfilled_tx.csv\", new_meanfilled_tx, delimiter=\",\")\n",
    "# np.savetxt(\"new_meanfilled_tx_test.csv\", new_meanfilled_tx_test, delimiter=\",\")\n",
    "\n",
    "new_tx = np.loadtxt(\"new_tx.csv\", delimiter=\",\")\n",
    "new_tx_test = np.loadtxt(\"new_tx_test.csv\", delimiter=\",\")\n",
    "new_zerofilled_tx = np.loadtxt(\"new_zerofilled_tx.csv\", delimiter=\",\")\n",
    "new_zerofilled_tx_test = np.loadtxt(\"new_zerofilled_tx_test.csv\", delimiter=\",\")\n",
    "new_meanfilled_tx = np.loadtxt(\"new_meanfilled_tx.csv\", delimiter=\",\")\n",
    "new_meanfilled_tx_test = np.loadtxt(\"new_meanfilled_tx_test.csv\", delimiter=\",\")\n",
    "new_all_tx = np.concatenate((new_tx, new_tx_test)) \n",
    "new_all_zerofilled_tx = np.concatenate((new_zerofilled_tx, new_zerofilled_tx_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datasets = [tx, new_meanfilled_tx, new_zerofilled_tx, new_tx]\n",
    "test_datasets = [tx_test, new_meanfilled_tx_test, new_zerofilled_tx_test, new_tx_test]\n",
    "datasets_names = ['Original/Raw', 'Mean filled', 'Zero filled', 'NaN dropped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gammas = np.logspace(-14, -18, 10)\n",
    "for i in range(len(train_datasets)):\n",
    "     logistic_regression_dataset_test(y, y_test, train_datasets[i], test_datasets[i], max_iters, gammas, datasets_names[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Logistic regression\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-14, -18, 10)# np.logspace(-16, -20, 10)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "weights = np.empty((0,new_meanfilled_tx.shape[1]), float)\n",
    "for gamma in np.nditer(gammas):\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    logistic_regression_loss, logistic_regression_w = logistic_regression(np.array([y]).T, new_meanfilled_tx, gamma, max_iters)\n",
    "    \n",
    "    # Print result\n",
    "    logistic_regression_loss = compute_rmse_loss(logistic_regression_loss)\n",
    "    train_losses = np.append(train_losses, logistic_regression_loss)\n",
    "    test_mse = compute_loss(y_test, tx_test, logistic_regression_w[:,0])\n",
    "    test_rmse = np.sqrt(2*test_mse)\n",
    "    test_losses = np.append(test_losses, test_rmse)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    weights = np.vstack((weights, logistic_regression_w.T))\n",
    "    print(\"Logistic Regression: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=logistic_regression_loss))\n",
    "\n",
    "\n",
    "plt.semilogx(gammas, test_losses, marker=\".\", color='r', label='test error')\n",
    "plt.semilogx(gammas, train_losses, marker=\".\", color='b', label='train error')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Least Squares \n",
    "start_time = datetime.datetime.now()\n",
    "least_squares_loss, least_squares_w = least_squares(y, new_tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "test_mse = compute_loss(y_test, new_tx_test, least_squares_w)\n",
    "test_rmse = np.sqrt(2*test_mse)\n",
    "# Print result\n",
    "least_squares_loss = compute_rmse_loss(least_squares_loss)\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Lest Squares: execution time={t:.3f} seconds. RMSE Train Loss={l}, Test Loss={tl}\".format(t=exection_time, l=least_squares_loss, tl=test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_weight = weights[2]\n",
    "print(best_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(new_tx[0:1000].shape)\n",
    "cov = np.cov(new_tx[0:1000].T)\n",
    "print(cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try to reduce dimensionality with PCA\n",
    "print(new_tx.shape)\n",
    "print(y.shape)\n",
    "tx_y = np.hstack((new_tx, np.array([y]).T))\n",
    "test_tx_y = np.hstack((new_all_tx, np.array([all_y]).T))\n",
    "print(tx_y.shape)\n",
    "print(test_tx_y.shape)\n",
    "\n",
    "results = PCA(tx_y) #this will return a 2d array of the data projected into PCA space\n",
    "test_results = PCA(test_tx_y)\n",
    "print(results.Y.shape)\n",
    "\n",
    "pca_tx = results.Y[:,0:10]\n",
    "pca_test_tx = test_results.Y[:,0:10]\n",
    "print(pca_tx.shape)\n",
    "print(pca_test_tx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "X = new_tx\n",
    "mu = np.mean(X, axis=0)\n",
    "\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "nComp = 20\n",
    "Xhat = np.dot(pca.transform(X)[:,:nComp], pca.components_[:nComp,:])\n",
    "Xhat += mu\n",
    "\n",
    "print(X[0])\n",
    "print(Xhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Why dont we try method that yielded best result so far: least squares\n",
    "\n",
    "# No improvement :(\n",
    "start_time = datetime.datetime.now()\n",
    "new_least_squares_loss, new_least_squares_w = lo(y, pca_tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "print(new_least_squares_w.shape)\n",
    "\n",
    "# Print result\n",
    "new_least_squares_loss = compute_rmse_loss(new_least_squares_loss)\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Lest Squares: execution time={t:.3f} seconds. RMSE Loss={l}\".format(t=exection_time, l=new_least_squares_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "subset_y = y\n",
    "subset_tx = new_all_zerofilled_tx\n",
    "\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "seed = 1\n",
    "k_fold = 10\n",
    "lambdas = np.logspace(-16, 2, 1)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "weights = np.empty((0,subset_tx.shape[1]), float)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for lambd in np.nditer(lambdas):\n",
    "    w, loss_tr, loss_te = cross_validation(subset_y, subset_tx, k_fold, seed, lambd)\n",
    "    rmse_tr = np.append(rmse_tr, loss_tr)\n",
    "    rmse_te = np.append(rmse_te, loss_te)\n",
    "    weights = np.vstack((weights, w))\n",
    "        \n",
    "end_time = datetime.datetime.now()\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"Cross Validation: execution time={t:.3f} seconds.\".format(t=exection_time))\n",
    "#cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(weights.shape)\n",
    "ls_w = weights[0,:]\n",
    "print(ls_w)\n",
    "test_mse = compute_loss(y_test, tx_test, ls_w)\n",
    "test_rmse = np.sqrt(2*test_mse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rmse_te[0])\n",
    "best_weights = weights[0]\n",
    "print(best_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test against local test set\n",
    "test_mse = compute_loss(y_test, tx_test, best_weight)\n",
    "test_rmse = np.sqrt(2*test_mse)\n",
    "\n",
    "print(test_mse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_tx = pca_test_tx[0:568238]\n",
    "print(pred_tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/logistic_regression_cross_validation_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "weights_pred = ls_w\n",
    "print(ls_w)\n",
    "y_pred = predict_labels(weights_pred, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://inclass.kaggle.com/c/epfml-project-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete train.csv such that github accepts push\n",
    "os.remove('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
