{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os,sys\n",
    "from PIL import Image, ImageOps, ImageChops\n",
    "from original_helpers import *\n",
    "from new_helpers import *\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "TOTAL_IMAGES = 50 # Number of images to load\n",
    "TRAIN_FRACTION = 0.9 # Percentage of images used for training\n",
    "ANGLE_STEP = 36 # Gotta be 360/ANGLE_STEP needs to be an integer\n",
    "FLIP = True # Flag to signal if flipped  versions of rotated images should also be created\n",
    "PATCH_SIZE = 16\n",
    "PATCH_TRANSLATION = 8 # WARNING: this quickly explodes to enormous amount of data if small patch_translation is selected.\n",
    "FOREGROUND_THRESHOLD = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "ORIGINAL_IMAGE_WIDTH = 400\n",
    "ORIGINAL_IMAGE_HEIGHT = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 images\n",
      "satImage_001.png\n",
      "Loading 50 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(TOTAL_IMAGES, len(files)) # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [img_float_to_uint8(load_image(image_dir + files[i])) for i in range(n)]\n",
    "\n",
    "train_size = int(TOTAL_IMAGES*TRAIN_FRACTION)\n",
    "train_imgs = imgs[0:train_size]\n",
    "test_imgs = imgs[train_size:]\n",
    "imgs_rotated_flipped = rotate_imgs(train_imgs, ANGLE_STEP, FLIP)\n",
    "train_imgs += imgs_rotated_flipped\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [img_float_to_uint8(load_image(gt_dir + files[i])) for i in range(n)]\n",
    "train_gt_imgs = gt_imgs[0:train_size]\n",
    "test_gt_imgs = gt_imgs[train_size:]\n",
    "gt_imgs_rotated_flipped = rotate_imgs(train_gt_imgs, ANGLE_STEP, FLIP)\n",
    "train_gt_imgs += gt_imgs_rotated_flipped\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def patch_range(w, h, w_translation, h_translation):\n",
    "    h_range = [0] if h_translation <= 0 else list(range(0, h, h_translation))\n",
    "    minus_h_range = [ x * -1 for x in h_range]\n",
    "    h_range = list(set(h_range + minus_h_range)) # trick to remove duplicates\n",
    "    h_range.sort()\n",
    "    \n",
    "    w_range = [0] if w_translation <= 0 else list(range(0, h, h_translation))\n",
    "    minus_w_range = [ x * -1 for x in w_range]\n",
    "    w_range = list(set(w_range + minus_w_range)) # trick to remove duplicates\n",
    "    w_range.sort()\n",
    "    \n",
    "    return h_range, w_range\n",
    "\n",
    "def img_crop(img, w, h, w_translation, h_translation):\n",
    "    list_patches = []\n",
    "    imgwidth = img.shape[0]\n",
    "    imgheight = img.shape[1]\n",
    "    is_2d = len(img.shape) < 3\n",
    "    if is_2d:\n",
    "        pil_img = Image.fromarray(img, 'L')\n",
    "    else:\n",
    "        pil_img = Image.fromarray(img, 'RGB')\n",
    "        \n",
    "    h_range, w_range = patch_range(w, h, w_translation, h_translation)\n",
    "    for it in h_range:\n",
    "        for jt in w_range:\n",
    "            \n",
    "            for i in range(0,imgheight,h):\n",
    "                for j in range(0,imgwidth,w):\n",
    "                    im_patch = pil_img.crop((j + jt, i + it, j + jt + w, i + it + h))\n",
    "                    list_patches.append(np.array(im_patch))\n",
    "    return list_patches\n",
    "\n",
    "def extract_patches(imgs, patch_size, patch_translation):\n",
    "    img_patches = [img_crop(imgs[i], patch_size, patch_size, patch_translation, patch_translation) for i in range(len(imgs))]\n",
    "    img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "    return img_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img_patches = extract_patches(train_imgs, PATCH_SIZE, PATCH_TRANSLATION)\n",
    "train_gt_patches = extract_patches(train_gt_imgs, PATCH_SIZE, PATCH_TRANSLATION)\n",
    "test_img_patches = extract_patches(test_imgs, PATCH_SIZE, PATCH_TRANSLATION)\n",
    "test_gt_patches = extract_patches(test_gt_imgs, PATCH_SIZE, PATCH_TRANSLATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute features for each image patch\n",
    "X = np.asarray([ extract_features(train_img_patches[i]) for i in range(len(train_img_patches))])\n",
    "Y = np.asarray([value_to_class(np.mean(train_gt_patches[i]), FOREGROUND_THRESHOLD) for i in range(len(train_gt_patches))])\n",
    "Xi = np.asarray([ extract_features(test_img_patches[i]) for i in range(len(test_img_patches))])\n",
    "Yi = np.asarray([value_to_class(np.mean(test_gt_patches[i]), FOREGROUND_THRESHOLD) for i in range(len(test_gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 4809375 features\n",
      "Feature dimension = 6\n",
      "Number of classes = 1\n",
      "Class 0: 3336085 samples\n",
      "Class 1: 1473290 samples\n"
     ]
    }
   ],
   "source": [
    "# Print feature statistics\n",
    "\n",
    "print('Computed ' + str(X.shape[0]) + ' features')\n",
    "print('Feature dimension = ' + str(X.shape[1]))\n",
    "print('Number of classes = ' + str(np.max(Y)))\n",
    "\n",
    "Y0 = [i for i, j in enumerate(Y) if j == 0]\n",
    "Y1 = [i for i, j in enumerate(Y) if j == 1]\n",
    "print('Class 0: ' + str(len(Y0)) + ' samples')\n",
    "print('Class 1: ' + str(len(Y1)) + ' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)  \n",
    "X_train = scaler.transform(X)\n",
    "clf = MLPClassifier()# default options\n",
    "clf.fit(X_train, Y)\n",
    "clf.score(X_train,Y)\n",
    "Xi = scaler.transform(Xi)  \n",
    "Zi = clf.predict(Xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "\n",
    "def unpatch(patches_array):\n",
    "    h_patches, w_patches = patch_range(PATCH_SIZE, PATCH_SIZE, PATCH_TRANSLATION, PATCH_TRANSLATION)\n",
    "    h_patches_rescaled = [x + abs(min(h_patches)) for x in h_patches]\n",
    "    w_patches_rescaled = [x + abs(min(w_patches)) for x in w_patches]\n",
    "    number_of_patches = (len(h_patches) * len(w_patches))\n",
    "    \n",
    "    number_of_pictures = patches_array.shape[0] / number_of_patches\n",
    "    if not number_of_pictures.is_integer() :\n",
    "        print('Something is wrong. Dimensions are incorrect!')\n",
    "        return\n",
    "    \n",
    "    unpatched_imgs = []\n",
    "    for img_no in range(int(number_of_pictures)):\n",
    "        pil_patched_image = Image.new('L', (ORIGINAL_IMAGE_WIDTH + w_patches_rescaled[-1], ORIGINAL_IMAGE_HEIGHT + h_patches_rescaled[-1]))\n",
    "        pil_index_image = pil_patched_image.copy()\n",
    "        \n",
    "        for j, h_patch in enumerate(h_patches_rescaled):\n",
    "            for i, w_patch in enumerate(w_patches_rescaled):\n",
    "                patch_mask = Image.new('L', (ORIGINAL_IMAGE_WIDTH + w_patches_rescaled[-1], ORIGINAL_IMAGE_HEIGHT + h_patches_rescaled[-1]))\n",
    "                index_mask = patch_mask.copy()\n",
    "\n",
    "                prediction_index = (img_no * number_of_patches) + j * len(h_patches_rescaled) + i\n",
    "                patch = np.ones((PATCH_SIZE, PATCH_SIZE), dtype='uint8')\n",
    "\n",
    "                prediction_patch = patch * patches_array[prediction_index]\n",
    "                index_patch = patch\n",
    "\n",
    "                pil_prediction_patch = Image.fromarray(prediction_patch, 'L')\n",
    "                pil_index_patch = Image.fromarray(index_patch, 'L')\n",
    "\n",
    "                upper_left_corner = (w_patch, h_patch)\n",
    "                patch_mask.paste(pil_prediction_patch, upper_left_corner)\n",
    "                index_mask.paste(pil_index_patch, upper_left_corner)\n",
    "\n",
    "                pil_patched_image = ImageChops.add(pil_patched_image, patch_mask)\n",
    "                pil_index_image = ImageChops.add(pil_index_image, index_mask)\n",
    "\n",
    "        patched_image = np.array(pil_patched_image)\n",
    "        index_image = np.array(pil_index_image)\n",
    "\n",
    "        h_from = h_patches[-1]\n",
    "        h_to = (ORIGINAL_IMAGE_HEIGHT + h_patches[-1])\n",
    "        w_from = w_patches[-1]\n",
    "        w_to = (ORIGINAL_IMAGE_WIDTH + w_patches[-1])\n",
    "        unpatched_img = np.around((patched_image/index_image))[h_from:h_to, w_from:w_to]\n",
    "        unpatched_imgs.append(unpatched_img)\n",
    "    return unpatched_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "unpached_Zi = unpatch(Zi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unpached_Zi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpached_Zi[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "unpached_Yi = unpatch(Yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unpached_Zi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpached_Yi[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67168871226051363"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(Yi, Zi, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
