"""
	 ****************** PCML: MINIPROJECT 2 ROAD SEGEMENTATION ******************
			authors: Mateusz Paluchowski, Marie Drieghe and Lazare Girardin
"""
import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D
from keras.layers import Convolution2D, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator

from helpers.data_helpers import extract_data_window, load_img_gt

"""
Notation Cheatsheet:
(ORIGINAL) - function provided by TAs and used in unmodified form
(MODIFIED) - function provided by TAs but modified for our purpose
(CUSTOM) - function created by us
"""

def train_cnn(model_name='dummy.h5', range_of_numbers):
	"""
		(CUSTOM)
		This function trains a convolutional neural network over input images. The  data input
		for the neural network are cropped patches of the images of size PATCH_SIZE.
		The labels used for training are the mean of the according patches taken from the groundtruth files.

		The CNN is trained using Keras, which is based on TensorFlow (or Theano if chosen, however some code changes would be needed).

		Args:
			- model_name (string): the name desired to store the resulting trained network
				Exemple: 'main_8x8.h5'
			- data_range ([]): range of images to load as a list containing beginning and end img
				Example: [1,100]
		Returns:
			- None. Saves trained model to a .h5 file in folder 'models/'.
	"""
	### TUNING PARAMETERS ###

	# Training batch size
	#	- could be increased to speed up training but
	#	- the higher it is the more memory it consumes
	batch_size = 128

	# Epochs to be trained
	nb_epoch = 55

	# number of convolutional filters to use
	nb_filters_layer1 = 64
	nb_filters_layer2 = 128

	# size of pooling area for max pooling
	pool_size = (2, 2)

	# convolution kernel size
	kernel_size_layer1 = (5, 5)
	kernel_size_layer2 = (3, 3)

	### GENERATE DATA ###
	PATCH_SIZE = 8
	WINDOW_SIZE = 17
	IMAGE_SIZE = 400
	TRAIN_RATIO = 0.8
	NUM_CHANNELS = 3

	# Output classes (road and rest)
	nb_classes = 2

	#TensorfFlow ordering. Won't work with Theano
	input_shape = (WINDOW_SIZE, WINDOW_SIZE, NUM_CHANNELS)

	# load dataset
	imgs, gts = load_img_gt(data_range)
	data, labels = extract_data_window(imgs, gts, PATCH_SIZE, WINDOW_SIZE, IMAGE_SIZE)

	# Create train and test sets
	idx = np.random.permutation(np.arange(data.shape[0]))
	train_size = int(TRAIN_RATIO*data.shape[0])
	X_train = data[idx[:train_size]]
	Y_train = labels[idx[:train_size]]
	X_test = data[idx[train_size:]]
	Y_test = labels[idx[train_size:]]

	### Neural Net ###

	model = Sequential()

	# Convolution layer with rectified linear activation
	model.add(Convolution2D(nb_filters_layer1, kernel_size_layer2[0], kernel_size_layer2[1], border_mode='same', input_shape=input_shape))

	model.add(Activation('relu'))

	# Second convolution
	model.add(Convolution2D(nb_filters_layer2, kernel_size_layer2[0],
							kernel_size_layer2[1]))
	model.add(Activation('relu'))

	# Third convolution
	model.add(Convolution2D(nb_filters_layer1, kernel_size_layer1[0],
							kernel_size_layer2[1]))
	model.add(Activation('relu'))

	# Pooling and dropout
	model.add(MaxPooling2D(pool_size=pool_size))
	model.add(SpatialDropout2D(0.25))

	# Full-connected layers
	model.add(Flatten())

	model.add(Dense(1024))
	model.add(Activation('relu'))
	model.add(Dropout(0.25))

	model.add(Dense(512))
	model.add(Activation('relu'))

	# Dropout to avoid overfitting
	model.add(Dropout(0.5))

	#Fully-connected layer to ouptut the resulting class
	model.add(Dense(nb_classes))
	model.add(Activation('softmax'))

	# Compile the model before training
	model.compile(loss='binary_crossentropy',
				optimizer='adadelta',
				metrics=['fmeasure'])

	# Keras' ingenious data generator allowing for real time data augmentation
	datagen = ImageDataGenerator(
		featurewise_center=False,  # set input mean to 0 over the dataset
		samplewise_center=False,  # set each sample mean to 0
		featurewise_std_normalization=False,  # divide inputs by std of the dataset
		samplewise_std_normalization=False,  # divide each input by its std
		zca_whitening=False,  # apply ZCA whitening
		rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)
		horizontal_flip=True,  # randomly flip images
		vertical_flip=False)  # randomly flip images

	# Compute quantities required for featurewise normalization
	# (std, mean, and principal components if ZCA whitening is applied).
	datagen.fit(X_train)

	# Fit the model on the batches generated by datagen.flow().
	model.fit_generator(datagen.flow(X_train, Y_train,
						batch_size=batch_size),
						samples_per_epoch=X_train.shape[0]*2, # *2 because we want to datagen do its magic but also don't spend ages on training
						nb_epoch=nb_epoch,
						class_weight='auto',
						verbose=1,
						validation_data=(X_test, Y_test))

	# Evaluate the model on the test set (excluded from training)
	score = model.evaluate(X_test, Y_test, verbose=0)
	print('Test score:', score[0])
	print('Test accuracy:', score[1])

	# Save the model
	model.save('models/' + model_name)

	return
