"""
	********* PCML: MINIPROJECT 2 ROAD SEGEMENTATION ***********************

	This function trains a convolutional neural network over some images. The input
	of the neural network are cropped patches of the images of size PATCH_SIZE.
	The labels used for training are the mean of the according patches taken from the groundtruth files.

	The CNN is trained using Keras, which is based on TensorFlow (or Theano if chosen).

	Function train_cnn():
		Input:
			-model_name: the name desired to store the resulting trained network
						 Exemple: 'main_8x8.h5'
		Ouput:
			-trained network in folder 'models/', as an .h5 file


	authors: Maateusz Paaluchowski, Marie Drieghe and Lazare Girardin
"""
import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D
from keras.layers import Convolution2D, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator

from data_helpers import extract_data_window, load_img_gt

def train_cnn(model_name='dummy.h5'):

	### TUNING PARAMETERS ###

	# Training batch size
	batch_size = 128

	# Epochs to be trained
	nb_epoch = 12

	# number of convolutional filters to use
	nb_filters_layer1 = 64
	nb_filters_layer2 = 128

	# size of pooling area for max pooling
	pool_size = (2, 2)

	# convolution kernel size
	kernel_size_layer1 = (5, 5)
	kernel_size_layer2 = (3, 3)

	### GENERATE DATA ###
	PATCH_SIZE = 8
	WINDOW_SIZE = 17
	IMAGE_SIZE = 400
	TRAIN_RATIO = 0.8
	NUM_CHANNELS = 3

	# Output classes (road and rest)
	nb_classes = 2

	input_shape = (WINDOW_SIZE, WINDOW_SIZE, NUM_CHANNELS)

	# load dataset
	imgs, gts = load_img_gt([1,10])
	data, labels = extract_data_window(imgs, gts, PATCH_SIZE, WINDOW_SIZE, IMAGE_SIZE)

	# Create train and test sets
	idx = np.random.permutation(np.arange(data.shape[0]))
	train_size = int(TRAIN_RATIO*data.shape[0])
	X_train = data[idx[:train_size]]
	Y_train = labels[idx[:train_size]]
	X_test = data[idx[train_size:]]
	Y_test = labels[idx[train_size:]]

	### Neural Net ###

	model = Sequential()

	# Convolution layer with rectified linear activation
	model.add(Convolution2D(nb_filters_layer1, kernel_size_layer2[0], kernel_size_layer2[1], border_mode='same', input_shape=input_shape))

	model.add(Activation('relu'))

	# Second convolution
	model.add(Convolution2D(nb_filters_layer2, kernel_size_layer2[0],
							kernel_size_layer2[1]))
	model.add(Activation('relu'))

	# Third convolution
	model.add(Convolution2D(nb_filters_layer1, kernel_size_layer1[0],
							kernel_size_layer2[1]))
	model.add(Activation('relu'))

	# Pooling and dropout
	model.add(MaxPooling2D(pool_size=pool_size))
	model.add(SpatialDropout2D(0.25))

	# Full-connected layers
	model.add(Flatten())

	model.add(Dense(1024))
	model.add(Activation('relu'))
	model.add(Dropout(0.25))

	model.add(Dense(512))
	model.add(Activation('relu'))

	# Dropout to avoid overfitting
	model.add(Dropout(0.5))

	#Fully-connected layer to ouptut the resulting class
	model.add(Dense(nb_classes))
	model.add(Activation('softmax'))

	# Compile the model before training
	model.compile(loss='binary_crossentropy',
				optimizer='adadelta',
				metrics=['fmeasure'])

	datagen = ImageDataGenerator(
		featurewise_center=False,  # set input mean to 0 over the dataset
		samplewise_center=False,  # set each sample mean to 0
		featurewise_std_normalization=False,  # divide inputs by std of the dataset
		samplewise_std_normalization=False,  # divide each input by its std
		zca_whitening=False,  # apply ZCA whitening
		rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)
		horizontal_flip=True,  # randomly flip images
		vertical_flip=False)  # randomly flip images

	# Compute quantities required for featurewise normalization
	# (std, mean, and principal components if ZCA whitening is applied).
	datagen.fit(X_train)

	# Fit the model on the batches generated by datagen.flow().
	model.fit_generator(datagen.flow(X_train, Y_train,
						batch_size=batch_size),
						samples_per_epoch=X_train.shape[0],
						nb_epoch=nb_epoch,
						class_weight='auto',
						verbose=1,
						validation_data=(X_test, Y_test))

	# Evaluate the model on the test set (excluded from training)
	score = model.evaluate(X_test, Y_test, verbose=0)
	print('Test score:', score[0])
	print('Test accuracy:', score[1])

	# Save the model
	model.save('models/' + model_name)

	return
