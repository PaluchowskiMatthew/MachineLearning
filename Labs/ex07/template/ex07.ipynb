{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "## Classification Using SVM\n",
    "Load dataset. We will re-use the CERN dataset from project 1, available from https://inclass.kaggle.com/c/epfml-project-1/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# TODO: convert labels to -1,1 ?\n",
    "\n",
    "## Note: This is the raw dataset, you can also work with your modified features if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 1)\n",
      "(250000, 30)\n",
      "(30, 250000)\n"
     ]
    }
   ],
   "source": [
    "y = np.array([y]).T\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "xt = x.T\n",
    "print(xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_cost(y, x, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\"\"\"\n",
    "    # Here x is the full dataset matrix, and y are the corresponding +1 or -1 labels\n",
    "    # ***************************************************\n",
    "    zeros = np.zeros((x.shape[0],1))\n",
    "    hinge_loss = np.ones((x.shape[0],1)) - y * np.dot(x, w)\n",
    "    #print(hinge_loss.shape)\n",
    "    hinge_loss = np.maximum(zeros, hinge_loss)\n",
    "    #print(hinge_loss.shape)\n",
    "    hinge_penalty = np.ones((y.shape[0],1)) * (lambda_/2) * np.linalg.norm(w[0])**2\n",
    "    #print(hinge_penalty.shape)\n",
    "    loss = np.sum(hinge_loss + hinge_penalty)\n",
    "#     print(loss)\n",
    "    # ***************************************************\n",
    "    return loss\n",
    "ww = np.zeros((x.shape[1],1))\n",
    "calculate_cost(y, x, ww, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the (stochastic) subgradient for the n-th summand of the SVM optimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(x[1].shape)\n",
    "print(y[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.60937000e+02],\n",
       "       [  6.87680000e+01],\n",
       "       [  1.03235000e+02],\n",
       "       [  4.81460000e+01],\n",
       "       [ -9.99000000e+02],\n",
       "       [ -9.99000000e+02],\n",
       "       [ -9.99000000e+02],\n",
       "       [  3.47300000e+00],\n",
       "       [  2.07800000e+00],\n",
       "       [  1.25157000e+02],\n",
       "       [  8.79000000e-01],\n",
       "       [  1.41400000e+00],\n",
       "       [ -9.99000000e+02],\n",
       "       [  4.20140000e+01],\n",
       "       [  2.03900000e+00],\n",
       "       [ -3.01100000e+00],\n",
       "       [  3.69180000e+01],\n",
       "       [  5.01000000e-01],\n",
       "       [  1.03000000e-01],\n",
       "       [  4.47040000e+01],\n",
       "       [ -1.91600000e+00],\n",
       "       [  1.64546000e+02],\n",
       "       [  1.00000000e+00],\n",
       "       [  4.62260000e+01],\n",
       "       [  7.25000000e-01],\n",
       "       [  1.15800000e+00],\n",
       "       [ -9.99000000e+02],\n",
       "       [ -9.99000000e+02],\n",
       "       [ -9.99000000e+02],\n",
       "       [  4.62260000e+01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gradient(y, x, w, lambda_, n):\n",
    "    \"\"\"compute the stochastic gradient of loss plus regularizer.\"\"\"\n",
    "    # Here x is one datapoint, and y is the corresponding +1 or -1 label\n",
    "    # \n",
    "    # ***************************************************\n",
    "    yy = np.array([y[n]])\n",
    "    xx = np.array([x[n]])\n",
    "#     print(yy.shape)\n",
    "#     print(xx.shape)\n",
    "\n",
    "    differentiable = 1 - yy * np.dot(xx, w)\n",
    "    if differentiable > 0:\n",
    "        hinge_loss_gd = -yy * xx\n",
    "    else:\n",
    "        hinge_loss_gd = 0 * xx\n",
    "    gradient = hinge_loss_gd.T + lambda_ * w\n",
    "    # ***************************************************\n",
    "#     hinge_loss_gd = 1 - yy * np.dot(xx, w)\n",
    "#     if hinge_loss_gd < 1:\n",
    "#         Ix = 1\n",
    "#     else:\n",
    "#         Ix = 0\n",
    "#     #hinge_loss_gd = np.maximum(zeros, hinge_loss_gd)\n",
    "#     gradient = w * Ix * (np.dot(xx, w)-yy) + lambda_ * w\n",
    "    # ***************************************************\n",
    "    #Classical MSE SGD\n",
    "#     e = np.dot(xx, w) - yy\n",
    "#     gradient = np.dot(xx.T, e) + (lambda_/2) * (w**2)\n",
    "    # ***************************************************\n",
    "    # Be careful about the constant N(size) term! The complete objective for SVM is a sum, not an average as in earlier SGD examples!\n",
    "    return gradient\n",
    "calculate_gradient(y, x, np.zeros((x.shape[1],1)), 0.1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent: Pick a data point uniformly at random and update w based on the gradient for the n-th summand of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=382326982.99186164\n",
      "Current iteration=100, the loss=109074954.78850296\n",
      "Current iteration=200, the loss=343502163.71438706\n",
      "Current iteration=300, the loss=263845570.77143487\n",
      "Current iteration=400, the loss=1193988179.7111957\n",
      "Current iteration=500, the loss=365468393.1370788\n",
      "Current iteration=600, the loss=223474406.450875\n",
      "Current iteration=700, the loss=400997183.0576159\n",
      "Current iteration=800, the loss=274590141.5101561\n",
      "Current iteration=900, the loss=109193261.03351654\n",
      "Objective = 187866347.29539007\n"
     ]
    }
   ],
   "source": [
    "def sgd_for_svm_demo(y, x):\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # classify the data by SGD for SVM: TODO\n",
    "    # ***************************************************\n",
    "    max_iter = 1000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 1.0 / y.shape[0]  # or set to a different value, try cross-validation!\n",
    "    \n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        # n = sample one data point uniformly at random data from x\n",
    "        n = int(np.random.uniform(low=0, high=x.shape[0]))\n",
    "        grad = calculate_gradient(y, x, w, lambda_, n)\n",
    "        w = w - (gamma * grad)\n",
    "        loss = calculate_cost(y, x, w, lambda_)\n",
    "        #print(loss)\n",
    "        \n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    \n",
    "    print(\"Objective = {l}\".format(l=calculate_cost(y, x, w, lambda_)))\n",
    "\n",
    "sgd_for_svm_demo(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent (Ascent) for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closed-form update for the n-th variable alpha, in the dual optimization problem, given alpha and the current corresponding w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.diag(y[:,0])\n",
    "print(np.dot(x, x.T).shape)\n",
    "# print(np.dot(x.T, Y).shape)\n",
    "# Q = np.dot(Y, np.dot(x , np.dot(x.T, Y)))\n",
    "# print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_coordinate_update(y, x, lambda_, n, w):\n",
    "    # Here x is one datapoint, and y is the corresponding +1 or -1 label\n",
    "    # ***************************************************\n",
    "    Y = np.diag(y[:,0])\n",
    "    Q = np.dot(Y, np.dot(x , np.dot(x.T, Y)))\n",
    "    a_t = np.array([x[n]])\n",
    "    alpha = a_t - 1/(2*lambda_) * (np.dot(a_t, np.dot(Q, a_t.T)))\n",
    "    # ***************************************************\n",
    "    return alpha\n",
    "calculate_coordinate_update(y, x, 0.1, 0, np.zeros((x.shape[1],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coordinate_descent_for_svm_demo(y, x):\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # classify the data by SGD for SVM: TODO\n",
    "    # ***************************************************\n",
    "    max_iter = 10000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 1.0 / y.shape[0]\n",
    "\n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        # n = uniformly random data point from x\n",
    "        n = int(np.random.uniform(low=0, high=x.shape[0]))\n",
    "        loss = calculate_cost(y, x, w, lamda_)\n",
    "        alpha = calculate_coordinate_update(y, x, lambda_, n, w)\n",
    "        \n",
    "        w = 1/lambda_ * np.dot(x.T, np.dot(Y, alpha)) # XYa\n",
    "        \n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    \n",
    "    print(\"Primal objective = {l}\".format(l=calculate_cost(y, x, w, lambda_)))\n",
    "\n",
    "coordinate_descent_for_svm_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
